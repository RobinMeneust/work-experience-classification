{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook to test SetFit performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup and data cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Change logs settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers.losses import CosineSimilarityLoss, BatchAllTripletLoss, BatchHardTripletLossDistanceFunction\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import logging\n",
    "import warnings\n",
    "\n",
    "# Disable some logs because there were too many messages during the tests\n",
    "# logging.disable(logging.INFO)\n",
    "# warnings.filterwarnings(\"ignore\", category=DeprecationWarning) \n",
    "\n",
    "from datasets import disable_progress_bar\n",
    "disable_progress_bar() # Disable the \"Map\" progress bar during the tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and clean the dataset\n",
    "\n",
    "This dataset is not on the GitHub repository.\n",
    "It's composed of work experienced fetched from LinkedIn and labelled between 0 and 4 (0 if it's not related to AI and 4 if it is)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Stagiaire ingénieur en intelligence artificiel...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Stagiaire en développement logiciel Développem...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Stagiaire en développement Web Création et évo...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Stagiaire en développement Web Portage d’une a...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Développeur Data / IA Développement d'applicat...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11281</th>\n",
       "      <td>Opérateur production Montage de transmission a...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11282</th>\n",
       "      <td>Opérateur production Montage de transmission a...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11283</th>\n",
       "      <td>Technicien réparation informatique Reparation ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11284</th>\n",
       "      <td>Technicien réparation Reparation &amp; maintenance...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11286</th>\n",
       "      <td>Développeur web freelance Webdesign &amp; Infographie</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5167 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text  label\n",
       "2      Stagiaire ingénieur en intelligence artificiel...      1\n",
       "3      Stagiaire en développement logiciel Développem...      0\n",
       "4      Stagiaire en développement Web Création et évo...      0\n",
       "5      Stagiaire en développement Web Portage d’une a...      0\n",
       "6      Développeur Data / IA Développement d'applicat...      1\n",
       "...                                                  ...    ...\n",
       "11281  Opérateur production Montage de transmission a...      0\n",
       "11282  Opérateur production Montage de transmission a...      0\n",
       "11283  Technicien réparation informatique Reparation ...      0\n",
       "11284  Technicien réparation Reparation & maintenance...      0\n",
       "11286  Développeur web freelance Webdesign & Infographie      0\n",
       "\n",
       "[5167 rows x 2 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataFrame = pd.read_pickle(r'../data/7587_corrige.pkl')\n",
    "subset = dataFrame[['jobTitle', 'description', 'label']].copy()\n",
    "\n",
    "subset.reset_index(drop=True, inplace=True)\n",
    "subset.replace('', np.nan, inplace=True)\n",
    "subset.dropna(inplace=True)\n",
    "\n",
    "subset['text'] = subset['jobTitle'] + ' ' + subset['description']\n",
    "subset = subset[['text','label']]\n",
    "subset_label_transform = subset.copy()\n",
    "\n",
    "subset_label_transform['label'] = np.where((subset_label_transform[\"label\"] < 3) | (subset_label_transform[\"label\"].isna()), 0, 1)\n",
    "subset_label_transform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split the dataset in two subsets : the training and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from benchmark.utility import split_dataset\n",
    "train_set, test_set = split_dataset(subset_label_transform, 0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\robin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from benchmark.utility import save_to_json\n",
    "from benchmark.tests import n_shot_tests, input_length_tests, distance_tests, loss_tests, language_tests, model_tests, num_epochs_tests, constant_params_tests, data_augmentation_tests\n",
    "from benchmark.train_eval_task import setfit_f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### N-shots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default SetFit uses the oversampling strategy and the Cosine Similarity loss. For instance if we have 8 positive and 8 negative examples then we have:\n",
    "\n",
    "|   | Y | Y | Y | Y | Y | Y | Y | Y | N | N | N | N | N | N | N | N |\n",
    "|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|\n",
    "| Y | + | + | + | + | + | + | + | + | - | - | - | - | - | - | - | - |\n",
    "| Y |   | + | + | + | + | + | + | + | - | - | - | - | - | - | - | - |\n",
    "| Y |   |   | + | + | + | + | + | + | - | - | - | - | - | - | - | - |\n",
    "| Y |   |   |   | + | + | + | + | + | - | - | - | - | - | - | - | - |\n",
    "| Y |   |   |   |   | + | + | + | + | - | - | - | - | - | - | - | - |\n",
    "| Y |   |   |   |   |   | + | + | + | - | - | - | - | - | - | - | - |\n",
    "| Y |   |   |   |   |   |   | + | + | - | - | - | - | - | - | - | - |\n",
    "| Y |   |   |   |   |   |   |   | + | - | - | - | - | - | - | - | - |\n",
    "| N |   |   |   |   |   |   |   |   | + | + | + | + | + | + | + | + |\n",
    "| N |   |   |   |   |   |   |   |   |   | + | + | + | + | + | + | + |\n",
    "| N |   |   |   |   |   |   |   |   |   |   | + | + | + | + | + | + |\n",
    "| N |   |   |   |   |   |   |   |   |   |   |   | + | + | + | + | + |\n",
    "| N |   |   |   |   |   |   |   |   |   |   |   |   | + | + | + | + |\n",
    "| N |   |   |   |   |   |   |   |   |   |   |   |   |   | + | + | + |\n",
    "| N |   |   |   |   |   |   |   |   |   |   |   |   |   |   | + | + |\n",
    "| N |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | + |\n",
    "\n",
    "- P = 2 * (8 + 7 + 6 + 5 + 4 + 3 + 2 + 1) \t= 72\n",
    "- N = 8 * 8 = 64 -> + 8 duplications \t\t= 72\n",
    "- Total = 72 + 72 = 144"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "g:\\CYTech\\ING2\\ProjetLinkedin\\2024_02_25\\linkedin-work-experience-classification\\.venv\\lib\\site-packages\\setfit\\data.py:154: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df = df.apply(lambda x: x.sample(min(num_samples, len(x)), random_state=seed))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 1 / 1 Estimated remaining time: ?\n"
     ]
    }
   ],
   "source": [
    "params = {\n",
    "    \"n_shot\": [1, 2, 4, 6, 10, 20, 40, 60, 100],\n",
    "    \"n_iter\": 15,\n",
    "    \"n_max_iter_per_shot\": 10,\n",
    "    \"model\": \"sentence-transformers/paraphrase-mpnet-base-v2\",\n",
    "    \"loss\": CosineSimilarityLoss\n",
    "}\n",
    "\n",
    "results, train_times, eval_times = n_shot_tests(params, train_set, test_set, few_shot_model_f1_function=setfit_f1_score)\n",
    "\n",
    "save_to_json(results, train_times, eval_times, params,  r'../results/setfit/n_shot')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    \"input_length_range\": [[0,5],[5,25],[25,50],[50,100],[100,200],[200,350]],\n",
    "    # [[6,10],[10,15],[15,20],[20,30], [6,15], [15,30], [6,20], [10,30], [6,30]],\n",
    "    # [[0,5],[5,10], [10,50], [50,100],[100,200],[200,350]],\n",
    "    # [[0,9],[1,9],[2,9],[3,9],[4,9],[5,9],[6,9],[7,9],[8,9],[9,9]],\n",
    "    # [[0,9],[9,100],[9,350],[100,350],[0,350]],\n",
    "\t# [[8,50],[8,100],[8,150],[8,200],[8,250],[8,300],[8,350]],\n",
    "\t# [[7,350],[8,350],[9,350],[10,350]],\n",
    "    # [[0,3],[0,4],[0,5],[0,6],[0,7],[0,8],[0,9],[0,10]],\n",
    "    # [[0,5],[0,10],[0,100],[6,100],[ 200,350]],\n",
    "\t\"n_shot\": 10,\n",
    "\t\"n_iter\": 100,\n",
    "\t\"model\": \"sentence-transformers/paraphrase-mpnet-base-v2\",\n",
    "\t\"loss\": CosineSimilarityLoss\n",
    "}\n",
    "\n",
    "results, train_times, eval_times = input_length_tests(params, train_set, test_set, few_shot_model_f1_function=setfit_f1_score)\n",
    "\n",
    "save_to_json(results, train_times, eval_times, params,  r'../results/setfit/input_length')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "\t\"n_shot\": 10,\n",
    "\t\"n_iter\": 100,\n",
    "\t\"model\": \"sentence-transformers/paraphrase-mpnet-base-v2\",\n",
    "\t\"distance\": {\n",
    "\t\t\"Cosine\":BatchHardTripletLossDistanceFunction.cosine_distance,\n",
    "\t\t\"Euclidian\": BatchHardTripletLossDistanceFunction.eucledian_distance, # it's really \"eucledian\" and not \"euclidian\" in the module sentence_transformers\n",
    "\t},\n",
    "\t\"loss\": CosineSimilarityLoss,\n",
    "}\n",
    "\n",
    "\n",
    "results, train_times, eval_times = distance_tests(params, train_set, test_set, few_shot_model_f1_function=setfit_f1_score)\n",
    "\n",
    "save_to_json(results, train_times, eval_times, params,  r'../results/setfit/distance')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss (pair-wise or Triplet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "\t\"n_shot\": 10,\n",
    "\t\"n_iter\": 100,\n",
    "\t\"model\": \"sentence-transformers/paraphrase-mpnet-base-v2\",\n",
    "\t\"loss\": {\"Pair-wise\":CosineSimilarityLoss, \"Triplet\":BatchAllTripletLoss}\n",
    "}\n",
    "\n",
    "results, train_times, eval_times = loss_tests(params, train_set, test_set, few_shot_model_f1_function=setfit_f1_score)\n",
    "\n",
    "save_to_json(results, train_times, eval_times, params,  r'../results/setfit/loss')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "\t\"n_shot\": 10,\n",
    "\t\"lang\": ['fr','en'],\n",
    "\t\"n_iter\": 100,\n",
    "\t\"model\": \"sentence-transformers/paraphrase-mpnet-base-v2\",\n",
    "\t\"loss\": CosineSimilarityLoss\n",
    "}\n",
    "\n",
    "results, train_times, eval_times = language_tests(params, train_set, test_set, few_shot_model_f1_function=setfit_f1_score)\n",
    "\n",
    "save_to_json(results, train_times, eval_times, params,  r'../results/setfit/language')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "\t\"n_shot\": 10,\n",
    "\t\"n_iter\": 100,\n",
    "\t\"loss\": CosineSimilarityLoss,\n",
    "\t\"model\": {\n",
    "\t\t# \"instructor-large\":\"hkunlp/instructor-large\",\n",
    "\t\t\"GIST-small-Embedding-v0\":\"avsolatorio/GIST-small-Embedding-v0\",\n",
    "\t\t\"gte-tiny\":\"TaylorAI/gte-tiny\",\n",
    "\t\t# \"all-mpnet-base-v2-table\":\"deepset/all-mpnet-base-v2-table\",\n",
    "  \t\t\"paraphrase-mpnet-base-v2\": \"sentence-transformers/paraphrase-mpnet-base-v2\",\n",
    "\t\t# \"all-mpnet-base-v2\":\"sentence-transformers/all-mpnet-base-v2\",\n",
    "\t}\n",
    "}\n",
    "results, run_train_times, eval_timestimes = model_tests(params, train_set, test_set, few_shot_model_f1_function=setfit_f1_score)\n",
    "\n",
    "save_to_json(results, train_times, eval_times, params,  r'../results/setfit/model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "\t\"n_shot\": 10,\n",
    "\t\"n_iter\": 100,\n",
    "\t\"loss\": CosineSimilarityLoss,\n",
    "\t\"model\": \"sentence-transformers/paraphrase-mpnet-base-v2\",\n",
    "\t\"num_epochs\": [(8,1),(8,2),(8,4),(8,8),(8,10),(8,20),(8,30),(8,40)], \n",
    "\t# [(1,1),(2,1),(4,1),(8,1),(16,1),(32,1),(64,1)], \n",
    "\t# [(1,1),(1,2),(1,4),(1,8),(1,12),(1,16),(1,20),(1,25),(1,30)],\n",
    "}\n",
    "\n",
    "results, train_times, eval_times = num_epochs_tests(params, train_set, test_set, few_shot_model_f1_function=setfit_f1_score)\n",
    "\n",
    "save_to_json(results, train_times, eval_times, params,  r'../results/setfit/num_epochs')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data sampling\n",
    "\n",
    "Run multiple tests with different training sets but the same parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# params = {\n",
    "# \t\"n_shot\": 10,\n",
    "# \t\"n_iter\": 100,\n",
    "# \t\"loss\": CosineSimilarityLoss,\n",
    "# \t\"model\": \"sentence-transformers/paraphrase-mpnet-base-v2\",\n",
    "# \t\"input_length_range\":[0,9],\n",
    "# }\n",
    "\n",
    "# results, run_times = constant_params_tests(params, train_set, test_set, few_shot_model_f1_function=setfit_f1_score)\n",
    "\n",
    "# save_to_json(results, run_times, params,  r'../results/setfit/data_sampling')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Augmentation\n",
    "\n",
    "For now we only use a back translation technique and synonym replacement, but we could try other ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    \"n_shot\": 10,\n",
    "    \"n_iter\": 100,\n",
    "    \"loss\": CosineSimilarityLoss,\n",
    "    \"model\": \"sentence-transformers/paraphrase-mpnet-base-v2\",\n",
    "    \"data_augmentation_ratio\": 1.3, # + 30 %\n",
    "    \"data_augmentation_strategy\":[\"none\",\"swapping_inter\", \"back_translation\", \"synonym_replacement\", \"crossover\"],\n",
    "    \"strategy_params\": {\n",
    "        \"n_points_crossover\": 2,\n",
    "        \"modification_rate\": 0.5,\n",
    "    }\n",
    "}\n",
    "\n",
    "results, train_times, eval_times = data_augmentation_tests(params, train_set, test_set, few_shot_model_f1_function=setfit_f1_score)\n",
    "save_to_json(results, train_times, eval_times, params,  r'../results/setfit/data_augmentation')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset label selection\n",
    "\n",
    "Here instead of considering Nan, 0, 1 and 2 as not being an AI experience and 3 and 4 as being one, we consider :\n",
    "\n",
    "- not AI = 0 and 1 and AI = 3 and 4 (we drop the examples with the label NaN or 2)\n",
    "- not AI = 0 and AI = 4 (we drop the examples with the label NaN, 1, 2 or 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_label_transform_likely_labels = subset.copy()\n",
    "subset_label_transform_likely_labels.replace({2: np.nan}, inplace=True)\n",
    "subset_label_transform_likely_labels.dropna(inplace=True)\n",
    "subset_label_transform_likely_labels['label'] = np.where((subset_label_transform_likely_labels[\"label\"] < 3), 0, 1)\n",
    "\n",
    "subset_label_transform_sure_labels = subset.copy()\n",
    "subset_label_transform_sure_labels.replace({1: np.nan, 2: np.nan, 3: np.nan}, inplace=True)\n",
    "subset_label_transform_sure_labels.dropna(inplace=True)\n",
    "subset_label_transform_sure_labels['label'] = np.where((subset_label_transform_sure_labels[\"label\"] == 0), 0, 1)\n",
    "\n",
    "# We keep the full test set\n",
    "train_set_likely_labels, _ = split_dataset(subset_label_transform_likely_labels, 0.2) \n",
    "train_set_sure_labels, _ = split_dataset(subset_label_transform_sure_labels, 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    \"n_shot\": 10,\n",
    "    \"n_iter\": 100,\n",
    "    \"loss\": CosineSimilarityLoss,\n",
    "    \"model\": \"sentence-transformers/paraphrase-mpnet-base-v2\",\n",
    "}\n",
    "\n",
    "tested_training_sets = {\n",
    "    \"all_labels\": train_set,\n",
    "    \"likely_labels\":train_set_likely_labels,\n",
    "    \"sure_labels\":train_set_sure_labels,\n",
    "}\n",
    "\n",
    "results = {}\n",
    "train_times = {}\n",
    "eval_times = {}\n",
    "progress = 0\n",
    "progress_end = len(tested_training_sets)\n",
    "\n",
    "for training_set_key, training_set_data in tested_training_sets.items():\n",
    "    print(\"Test: \", progress,\"/\",progress_end)\n",
    "    temp_results, temp_train_times, temp_eval_times = constant_params_tests(params, training_set_data, test_set, setfit_f1_score)\n",
    "    results[training_set_key] = temp_results[\"all\"]\n",
    "    train_times[training_set_key] = temp_train_times[\"all\"]\n",
    "    eval_times[training_set_key] = temp_eval_times[\"all\"]\n",
    "\n",
    "params[\"training_set\"] = list(tested_training_sets.keys())\n",
    "save_to_json(results, train_times, eval_times, params,  r'../results/setfit/training_set_labels_restriction')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
