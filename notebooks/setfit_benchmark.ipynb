{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook to test SetFit performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup and data cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Change logs settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers.losses import CosineSimilarityLoss, BatchAllTripletLoss, BatchHardTripletLossDistanceFunction\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import logging\n",
    "import warnings\n",
    "\n",
    "# Disable some logs because there were too many messages during the tests\n",
    "# logging.disable(logging.INFO)\n",
    "# warnings.filterwarnings(\"ignore\", category=DeprecationWarning) \n",
    "\n",
    "from datasets import disable_progress_bar\n",
    "disable_progress_bar() # Disable the \"Map\" progress bar during the tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and clean the dataset\n",
    "\n",
    "This dataset is not on the GitHub repository.\n",
    "It's composed of work experienced fetched from LinkedIn and labelled between 0 and 4 (0 if it's not related to AI and 4 if it is)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataFrame = pd.read_pickle(r'../data/7587_corrige.pkl')\n",
    "subset = dataFrame[['jobTitle', 'description', 'label']].copy()\n",
    "\n",
    "subset.reset_index(drop=True, inplace=True)\n",
    "subset.replace('', np.nan, inplace=True) # drop NaN labels, job titles and descriptions\n",
    "subset.dropna(inplace=True)\n",
    "\n",
    "subset['text'] = subset['jobTitle'] + '. ' + subset['description']\n",
    "subset = subset[['text','label']]\n",
    "subset_label_transform = subset.copy()\n",
    "\n",
    "subset_label_transform['label'] = np.where((subset_label_transform[\"label\"] < 3), 0, 1)\n",
    "subset_label_transform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split the dataset in two subsets : the training and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from benchmark.utility import split_dataset\n",
    "train_set, test_set = split_dataset(subset_label_transform, 0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from benchmark.utility import save_to_json\n",
    "from benchmark.tests import n_shot_tests, input_length_tests, distance_tests, loss_tests, language_tests, model_tests, num_epochs_tests, constant_params_tests, data_augmentation_tests\n",
    "from benchmark.train_eval_task import setfit_f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### N-shots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default SetFit uses the oversampling strategy and the Cosine Similarity loss. For instance if we have 8 positive and 8 negative examples then we have:\n",
    "\n",
    "|   | Y | Y | Y | Y | Y | Y | Y | Y | N | N | N | N | N | N | N | N |\n",
    "|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|\n",
    "| Y | + | + | + | + | + | + | + | + | - | - | - | - | - | - | - | - |\n",
    "| Y |   | + | + | + | + | + | + | + | - | - | - | - | - | - | - | - |\n",
    "| Y |   |   | + | + | + | + | + | + | - | - | - | - | - | - | - | - |\n",
    "| Y |   |   |   | + | + | + | + | + | - | - | - | - | - | - | - | - |\n",
    "| Y |   |   |   |   | + | + | + | + | - | - | - | - | - | - | - | - |\n",
    "| Y |   |   |   |   |   | + | + | + | - | - | - | - | - | - | - | - |\n",
    "| Y |   |   |   |   |   |   | + | + | - | - | - | - | - | - | - | - |\n",
    "| Y |   |   |   |   |   |   |   | + | - | - | - | - | - | - | - | - |\n",
    "| N |   |   |   |   |   |   |   |   | + | + | + | + | + | + | + | + |\n",
    "| N |   |   |   |   |   |   |   |   |   | + | + | + | + | + | + | + |\n",
    "| N |   |   |   |   |   |   |   |   |   |   | + | + | + | + | + | + |\n",
    "| N |   |   |   |   |   |   |   |   |   |   |   | + | + | + | + | + |\n",
    "| N |   |   |   |   |   |   |   |   |   |   |   |   | + | + | + | + |\n",
    "| N |   |   |   |   |   |   |   |   |   |   |   |   |   | + | + | + |\n",
    "| N |   |   |   |   |   |   |   |   |   |   |   |   |   |   | + | + |\n",
    "| N |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | + |\n",
    "\n",
    "- P = 2 * (8 + 7 + 6 + 5 + 4 + 3 + 2 + 1) \t= 72\n",
    "- N = 8 * 8 = 64 -> + 8 duplications \t\t= 72\n",
    "- Total = 72 + 72 = 144"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    \"n_shot\": [1, 2, 4, 6, 10, 20, 40, 60, 100],\n",
    "    \"n_iter\": 10,\n",
    "    \"n_max_iter_per_shot\": 10,\n",
    "    \"model\": \"sentence-transformers/paraphrase-mpnet-base-v2\",\n",
    "    \"loss\": BatchAllTripletLoss\n",
    "}\n",
    "\n",
    "results, train_times, eval_times = n_shot_tests(params, train_set, test_set, few_shot_model_f1_function=setfit_f1_score)\n",
    "\n",
    "save_to_json(results, train_times, eval_times, params,  r'../results/setfit/n_shot')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    \"input_length_range\": [[0,5],[5,25],[25,50],[50,100],[100,200],[200,350]],\n",
    "    # [[6,10],[10,15],[15,20],[20,30], [6,15], [15,30], [6,20], [10,30], [6,30]],\n",
    "    # [[0,5],[5,10], [10,50], [50,100],[100,200],[200,350]],\n",
    "    # [[0,9],[1,9],[2,9],[3,9],[4,9],[5,9],[6,9],[7,9],[8,9],[9,9]],\n",
    "    # [[0,9],[9,100],[9,350],[100,350],[0,350]],\n",
    "\t# [[8,50],[8,100],[8,150],[8,200],[8,250],[8,300],[8,350]],\n",
    "\t# [[7,350],[8,350],[9,350],[10,350]],\n",
    "    # [[0,3],[0,4],[0,5],[0,6],[0,7],[0,8],[0,9],[0,10]],\n",
    "    # [[0,5],[0,10],[0,100],[6,100],[ 200,350]],\n",
    "\t\"n_shot\": 8,\n",
    "\t\"n_iter\": 50,\n",
    "\t\"model\": \"sentence-transformers/paraphrase-mpnet-base-v2\",\n",
    "\t\"loss\": BatchAllTripletLoss\n",
    "}\n",
    "\n",
    "results, train_times, eval_times = input_length_tests(params, train_set, test_set, few_shot_model_f1_function=setfit_f1_score)\n",
    "\n",
    "save_to_json(results, train_times, eval_times, params,  r'../results/setfit/input_length')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "\t\"n_shot\": 8,\n",
    "\t\"n_iter\": 50,\n",
    "\t\"model\": \"sentence-transformers/paraphrase-mpnet-base-v2\",\n",
    "\t\"distance\": {\n",
    "\t\t\"Cosine\":BatchHardTripletLossDistanceFunction.cosine_distance,\n",
    "\t\t\"Euclidian\": BatchHardTripletLossDistanceFunction.eucledian_distance, # it's really \"eucledian\" and not \"euclidian\" in the module sentence_transformers\n",
    "\t},\n",
    "\t\"loss\": BatchAllTripletLoss,\n",
    "}\n",
    "\n",
    "\n",
    "results, train_times, eval_times = distance_tests(params, train_set, test_set, few_shot_model_f1_function=setfit_f1_score)\n",
    "\n",
    "save_to_json(results, train_times, eval_times, params,  r'../results/setfit/distance')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss (pair-wise or Triplet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "\t\"n_shot\": 8,\n",
    "\t\"n_iter\": 50,\n",
    "\t\"model\": \"sentence-transformers/paraphrase-mpnet-base-v2\",\n",
    "\t\"loss\": {\"Pair-wise\":CosineSimilarityLoss, \"Triplet\":BatchAllTripletLoss}\n",
    "}\n",
    "\n",
    "results, train_times, eval_times = loss_tests(params, train_set, test_set, few_shot_model_f1_function=setfit_f1_score)\n",
    "\n",
    "save_to_json(results, train_times, eval_times, params,  r'../results/setfit/loss')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "\t\"n_shot\": 8,\n",
    "\t\"lang\": ['fr','en'],\n",
    "\t\"n_iter\": 50,\n",
    "\t\"model\": \"sentence-transformers/paraphrase-mpnet-base-v2\",\n",
    "\t\"loss\": BatchAllTripletLoss\n",
    "}\n",
    "\n",
    "results, train_times, eval_times = language_tests(params, train_set, test_set, few_shot_model_f1_function=setfit_f1_score)\n",
    "\n",
    "save_to_json(results, train_times, eval_times, params,  r'../results/setfit/language')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "\t\"n_shot\": 8,\n",
    "\t\"n_iter\": 50,\n",
    "\t\"loss\": BatchAllTripletLoss,\n",
    "\t\"model\": {\n",
    "\t\t# \"instructor-large\":\"hkunlp/instructor-large\",\n",
    "\t\t\"GIST-small-Embedding-v0\":\"avsolatorio/GIST-small-Embedding-v0\",\n",
    "\t\t\"gte-tiny\":\"TaylorAI/gte-tiny\",\n",
    "\t\t# \"all-mpnet-base-v2-table\":\"deepset/all-mpnet-base-v2-table\",\n",
    "  \t\t\"paraphrase-mpnet-base-v2\": \"sentence-transformers/paraphrase-mpnet-base-v2\",\n",
    "\t\t# \"all-mpnet-base-v2\":\"sentence-transformers/all-mpnet-base-v2\",\n",
    "\t}\n",
    "}\n",
    "results, run_train_times, eval_timestimes = model_tests(params, train_set, test_set, few_shot_model_f1_function=setfit_f1_score)\n",
    "\n",
    "save_to_json(results, train_times, eval_times, params,  r'../results/setfit/model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "\t\"n_shot\": 8,\n",
    "\t\"n_iter\": 50,\n",
    "\t\"loss\": BatchAllTripletLoss,\n",
    "\t\"model\": \"sentence-transformers/paraphrase-mpnet-base-v2\",\n",
    "\t\"num_epochs\": [(8,1),(8,2),(8,4),(8,8),(8,10),(8,20),(8,30),(8,40)], \n",
    "\t# [(1,1),(2,1),(4,1),(8,1),(16,1),(32,1),(64,1)], \n",
    "\t# [(1,1),(1,2),(1,4),(1,8),(1,12),(1,16),(1,20),(1,25),(1,30)],\n",
    "}\n",
    "\n",
    "results, train_times, eval_times = num_epochs_tests(params, train_set, test_set, few_shot_model_f1_function=setfit_f1_score)\n",
    "\n",
    "save_to_json(results, train_times, eval_times, params,  r'../results/setfit/num_epochs')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data sampling\n",
    "\n",
    "Run multiple tests with different training sets but the same parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "\t\"n_shot\": 10,\n",
    "\t\"n_iter\": 50,\n",
    "\t\"loss\": CosineSimilarityLoss,\n",
    "\t\"model\": \"sentence-transformers/paraphrase-mpnet-base-v2\",\n",
    "}\n",
    "\n",
    "results, train_times, eval_times = constant_params_tests(params, train_set, test_set, few_shot_model_f1_function=setfit_f1_score)\n",
    "\n",
    "save_to_json(results, train_times, eval_times, params,  r'../results/setfit/data_sampling')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    \"n_shot\": 8,\n",
    "    \"n_iter\": 50,\n",
    "    \"loss\": CosineSimilarityLoss,\n",
    "    \"model\": \"sentence-transformers/paraphrase-mpnet-base-v2\",\n",
    "    \"data_augmentation_ratio\": 1.3, # + 30 %\n",
    "    \"data_augmentation_strategy\":[\"none\",\"swapping_inter\", \"back_translation\", \"synonym_replacement\", \"crossover\"],\n",
    "    \"strategy_params\": {\n",
    "        \"n_points_crossover\": 2,\n",
    "        \"modification_rate\": 0.5,\n",
    "    }\n",
    "}\n",
    "\n",
    "results, train_times, eval_times = data_augmentation_tests(params, train_set, test_set, few_shot_model_f1_function=setfit_f1_score)\n",
    "save_to_json(results, train_times, eval_times, params,  r'../results/setfit/data_augmentation')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset label selection\n",
    "\n",
    "Here instead of considering Nan, 0, 1 and 2 as not being an AI experience and 3 and 4 as being one, we consider :\n",
    "\n",
    "- not AI = 0 and 1 and AI = 3 and 4 (we drop the examples with the label NaN or 2)\n",
    "- not AI = 0 and AI = 4 (we drop the examples with the label NaN, 1, 2 or 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_label_transform_likely_labels = subset.copy()\n",
    "subset_label_transform_likely_labels.replace({2: np.nan}, inplace=True)\n",
    "subset_label_transform_likely_labels.dropna(inplace=True)\n",
    "subset_label_transform_likely_labels['label'] = np.where((subset_label_transform_likely_labels[\"label\"] < 3), 0, 1)\n",
    "\n",
    "subset_label_transform_sure_labels = subset.copy()\n",
    "subset_label_transform_sure_labels.replace({1: np.nan, 2: np.nan, 3: np.nan}, inplace=True)\n",
    "subset_label_transform_sure_labels.dropna(inplace=True)\n",
    "subset_label_transform_sure_labels['label'] = np.where((subset_label_transform_sure_labels[\"label\"] == 0), 0, 1)\n",
    "\n",
    "# We keep the full test set\n",
    "train_set_likely_labels, _ = split_dataset(subset_label_transform_likely_labels, 0.2) \n",
    "train_set_sure_labels, _ = split_dataset(subset_label_transform_sure_labels, 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    \"n_shot\": 10,\n",
    "    \"n_iter\": 50,\n",
    "    \"loss\": CosineSimilarityLoss,\n",
    "    \"model\": \"sentence-transformers/paraphrase-mpnet-base-v2\",\n",
    "}\n",
    "\n",
    "tested_training_sets = {\n",
    "    \"all_labels\": train_set,\n",
    "    \"likely_labels\":train_set_likely_labels,\n",
    "    \"sure_labels\":train_set_sure_labels,\n",
    "}\n",
    "\n",
    "results = {}\n",
    "train_times = {}\n",
    "eval_times = {}\n",
    "progress = 0\n",
    "progress_end = len(tested_training_sets)\n",
    "\n",
    "for training_set_key, training_set_data in tested_training_sets.items():\n",
    "    print(\"Test: \", progress,\"/\",progress_end)\n",
    "    temp_results, temp_train_times, temp_eval_times = constant_params_tests(params, training_set_data, test_set, setfit_f1_score)\n",
    "    results[training_set_key] = temp_results[\"all\"]\n",
    "    train_times[training_set_key] = temp_train_times[\"all\"]\n",
    "    eval_times[training_set_key] = temp_eval_times[\"all\"]\n",
    "\n",
    "params[\"training_set\"] = list(tested_training_sets.keys())\n",
    "save_to_json(results, train_times, eval_times, params,  r'../results/setfit/training_set_labels_restriction')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
