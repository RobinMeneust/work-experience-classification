{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook to test how to use BERT and PyTorch with CUDA for FSL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check CUDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "if torch.cuda.is_available():    \n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 768])\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n",
    "bert = BertModel.from_pretrained(\"bert-base-multilingual-cased\")\n",
    "bert.to(device)\n",
    "bert.cuda()\n",
    "\n",
    "encoded_input = tokenizer([\"test 1\",\"test 2\"], return_tensors='pt')\n",
    "encoded_input.to(device)\n",
    "output = bert(**encoded_input)\n",
    "print(output[\"pooler_output\"].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load dataset and transform data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "dataFrame = pd.read_pickle(r'../data/7587_corrige.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Stagiaire ingénieur en intelligence artificiel...</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Stagiaire en développement logiciel Développem...</td>\n",
       "      <td>0.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Stagiaire en développement Web Création et évo...</td>\n",
       "      <td>0.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Stagiaire en développement Web Portage d’une a...</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Développeur Data / IA Développement d'applicat...</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11281</th>\n",
       "      <td>Opérateur production Montage de transmission a...</td>\n",
       "      <td>0.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11282</th>\n",
       "      <td>Opérateur production Montage de transmission a...</td>\n",
       "      <td>0.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11283</th>\n",
       "      <td>Technicien réparation informatique Reparation ...</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11284</th>\n",
       "      <td>Technicien réparation Reparation &amp; maintenance...</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11286</th>\n",
       "      <td>Développeur web freelance Webdesign &amp; Infographie</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5167 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text  label\n",
       "2      Stagiaire ingénieur en intelligence artificiel...   1.00\n",
       "3      Stagiaire en développement logiciel Développem...   0.50\n",
       "4      Stagiaire en développement Web Création et évo...   0.25\n",
       "5      Stagiaire en développement Web Portage d’une a...   0.00\n",
       "6      Développeur Data / IA Développement d'applicat...   1.00\n",
       "...                                                  ...    ...\n",
       "11281  Opérateur production Montage de transmission a...   0.25\n",
       "11282  Opérateur production Montage de transmission a...   0.25\n",
       "11283  Technicien réparation informatique Reparation ...   0.00\n",
       "11284  Technicien réparation Reparation & maintenance...   0.00\n",
       "11286  Développeur web freelance Webdesign & Infographie   0.00\n",
       "\n",
       "[5167 rows x 2 columns]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subset = dataFrame[['jobTitle', 'description', 'label']].copy()\n",
    "\n",
    "subset.reset_index(drop=True, inplace=True)\n",
    "subset.replace('', np.nan, inplace=True)\n",
    "subset.dropna(inplace=True)\n",
    "\n",
    "subset['text'] = subset['jobTitle'] + ' ' + subset['description']\n",
    "subset['label'] = subset['label']/4.0\n",
    "subset = subset[['text','label']]\n",
    "subset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get text embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_shots = 10 # Number of samples per class in the support set\n",
    "\n",
    "def gen_support_set(n_shots, tokenizer, dataset):   \n",
    "    shuffled_dataset = dataset.sample(frac = 1)\n",
    "    support_set = {}\n",
    "    for t in [0,1]: # class 0 and class 1 (not related to AI and related to AI)\n",
    "        current_target_dataset = shuffled_dataset[shuffled_dataset[\"label\"] == t]\n",
    "        support_set[t] = []\n",
    "        for i in range(n_shots):\n",
    "            encoded_input = tokenizer(current_target_dataset.iloc[i][\"text\"], return_tensors='pt', truncation=True)\n",
    "            encoded_input.to(device)\n",
    "            support_set[t].append(encoded_input)\n",
    "    return support_set\n",
    "    \n",
    "support_set = gen_support_set(n_shots, tokenizer, subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embeddings_support_set(support_set, bert):\n",
    "    embeddings_support_set = {}\n",
    "    for t in support_set.keys():\n",
    "        embeddings_support_set[t] = []\n",
    "        for i in range(len(support_set[t])):\n",
    "            output = bert(**(support_set[t][i]))[\"pooler_output\"]\n",
    "            embeddings_support_set[t].append(output)\n",
    "    return embeddings_support_set\n",
    "\n",
    "embeddings_support_set = get_embeddings_support_set(support_set, bert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "def predict(tokenizer, bert, instance, support_set):\n",
    "    bert.eval()\n",
    "    encoded_input = tokenizer(instance, return_tensors='pt', truncation=True)\n",
    "    encoded_input.to(device)\n",
    "    embedding = bert(**encoded_input)[\"pooler_output\"]\n",
    "    similarities = []\n",
    "    \n",
    "    embeddings_support_set = get_embeddings_support_set(support_set, bert)\n",
    "    \n",
    "    for key in embeddings_support_set.keys():\n",
    "        similarities_current_key = []\n",
    "        for item in embeddings_support_set[key]:\n",
    "            similarity = torch.nn.functional.cosine_similarity(embedding, item)\n",
    "            similarities_current_key.append(torch.mean(similarity))\n",
    "        similarities.append(torch.max(torch.stack(similarities_current_key))) # Take the closest element of the support set for the class key to the input\n",
    "    return list(embeddings_support_set.keys())[torch.argmax(torch.stack(similarities))] # Take the closest element of all classes and return its class label\n",
    "\n",
    "print(predict(tokenizer, bert, subset.iloc[0][\"text\"], support_set))\n",
    "print(subset.iloc[0][\"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_batches(training_set, tokenizer, batch_size):\n",
    "    batches = []\n",
    "    shuffled_set = training_set.sample(frac=1)\n",
    "\n",
    "    nb_batches = len(shuffled_set) // batch_size\n",
    "    \n",
    "    k = 0\n",
    "    len_shuffled_set = len(shuffled_set)\n",
    "    unprocessed_data = shuffled_set[\"text\"].tolist()\n",
    "    \n",
    "    for i in range(nb_batches):\n",
    "        j = 0\n",
    "        labels = []\n",
    "        start = i * batch_size\n",
    "        end = start + batch_size\n",
    "        unprocessed_batch = unprocessed_data[start:end]\n",
    "        inputs = tokenizer(unprocessed_batch, return_tensors='pt', padding=True, truncation=True)\n",
    "\n",
    "        while(j<batch_size and k<len_shuffled_set):\n",
    "            labels.append(shuffled_set.iloc[k][\"label\"])\n",
    "            k += 1\n",
    "            j += 1\n",
    "        batches.append((inputs, labels))\n",
    "            \n",
    "    return batches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_train_test(dataset, ratio):\n",
    "    test_set = dataset.sample(frac = ratio)\n",
    "    train_set = dataset.drop(test_set.index)\n",
    "    return train_set, test_set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Freeze some weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "freeze_first_params_ratio = 0.7\n",
    "nb_frozen_params = int(freeze_first_params_ratio * len(list(bert.named_parameters())))\n",
    "\n",
    "for name, param in list(bert.named_parameters())[0:nb_frozen_params+1]: \n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fine-tune BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0 / 30\n",
      "loss: 0.60\n",
      "Epoch:  1 / 30\n",
      "loss: 0.50\n",
      "Epoch:  2 / 30\n",
      "loss: 0.47\n",
      "Epoch:  3 / 30\n",
      "loss: 0.44\n",
      "Epoch:  4 / 30\n",
      "loss: 0.41\n",
      "Epoch:  5 / 30\n",
      "loss: 0.31\n",
      "Epoch:  6 / 30\n",
      "loss: 0.34\n",
      "Epoch:  7 / 30\n",
      "loss: 0.30\n",
      "Epoch:  8 / 30\n",
      "loss: 0.28\n",
      "Epoch:  9 / 30\n",
      "loss: 0.26\n",
      "Epoch:  10 / 30\n",
      "loss: 0.25\n",
      "Epoch:  11 / 30\n",
      "loss: 0.26\n",
      "Epoch:  12 / 30\n",
      "loss: 0.25\n",
      "Epoch:  13 / 30\n",
      "loss: 0.21\n",
      "Epoch:  14 / 30\n",
      "loss: 0.22\n",
      "Epoch:  15 / 30\n",
      "loss: 0.21\n",
      "Epoch:  16 / 30\n",
      "loss: 0.24\n",
      "Epoch:  17 / 30\n",
      "loss: 0.22\n",
      "Epoch:  18 / 30\n",
      "loss: 0.21\n",
      "Epoch:  19 / 30\n",
      "loss: 0.21\n",
      "Epoch:  20 / 30\n",
      "loss: 0.22\n",
      "Epoch:  21 / 30\n",
      "loss: 0.19\n",
      "Epoch:  22 / 30\n",
      "loss: 0.17\n",
      "Epoch:  23 / 30\n",
      "loss: 0.21\n",
      "Epoch:  24 / 30\n",
      "loss: 0.20\n",
      "Epoch:  25 / 30\n",
      "loss: 0.20\n",
      "Epoch:  26 / 30\n",
      "loss: 0.18\n",
      "Epoch:  27 / 30\n",
      "loss: 0.19\n",
      "Epoch:  28 / 30\n",
      "loss: 0.18\n",
      "Epoch:  29 / 30\n",
      "loss: 0.17\n"
     ]
    }
   ],
   "source": [
    "subset_trunc = subset.head(100)\n",
    "train_set, test_set = split_train_test(subset_trunc, 0.2)\n",
    "n_epochs = 30\n",
    "optimizer = torch.optim.AdamW(bert.parameters(), lr=1e-5)\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "bert.zero_grad()\n",
    "\n",
    "try:\n",
    "    bert.train()\n",
    "    for epoch in range(n_epochs):\n",
    "        batches = gen_batches(train_set, tokenizer, 16)\n",
    "        print(\"Epoch: \", epoch, \"/\",n_epochs)\n",
    "        #b = 0\n",
    "        epoch_mean_loss = 0\n",
    "        for batch in batches:\n",
    "            optimizer.zero_grad()\n",
    "            inputs, labels = batch\n",
    "            #print(\"Batch: \", b, \"/\",len(batches))\n",
    "            #b += 1\n",
    "            predictions = []\n",
    "            inputs.to(device)\n",
    "            bert_output = bert(**inputs)[\"pooler_output\"]\n",
    "            losses = []\n",
    "            \n",
    "            \n",
    "            embeddings_support_set = get_embeddings_support_set(support_set, bert)\n",
    "\t\t\n",
    "            for i in range(len(bert_output)):\n",
    "                input2 = torch.unsqueeze(bert_output[i],0)\n",
    "                input2.to(device)\n",
    "                for j in embeddings_support_set.keys():\n",
    "                    current_class_support_data = embeddings_support_set[j]\n",
    "                    target = torch.tensor([1.0]) if j == labels[i] else torch.tensor([-1.0])\n",
    "                    target = target.to(device)\n",
    "                    for n in range(n_shots):\n",
    "                        losses.append(torch.nn.functional.cosine_embedding_loss(current_class_support_data[n], input2, target))\n",
    "                    \n",
    "            loss = torch.mean(torch.stack(losses))\n",
    "            epoch_mean_loss += loss.item()\n",
    "                        \n",
    "            #torch.nn.utils.clip_grad_norm_(bert.parameters(), 1.0)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        epoch_mean_loss /= len(batches)\n",
    "        print(f\"loss: {epoch_mean_loss:.2f}\")\n",
    "finally:\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "print(predict(tokenizer, bert, subset.iloc[0][\"text\"], support_set))\n",
    "print(subset.iloc[0][\"label\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
