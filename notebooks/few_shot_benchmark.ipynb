{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook to test SetFit performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup and data cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check if CUDA is available and use it if it is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from sentence_transformers.losses import CosineSimilarityLoss, BatchAllTripletLoss, BatchHardTripletLossDistanceFunction\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import logging\n",
    "import warnings\n",
    "\n",
    "# Disable some logs because there were too many messages during the tests\n",
    "# logging.disable(logging.INFO)\n",
    "# warnings.filterwarnings(\"ignore\", category=DeprecationWarning) \n",
    "\n",
    "from datasets import disable_progress_bar\n",
    "disable_progress_bar() # Disable the \"Map\" progress bar during the tests\n",
    "    \n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and clean the dataset\n",
    "\n",
    "This dataset is not on the GitHub repository.\n",
    "It's composed of work experienced fetched from LinkedIn and labelled between 0 and 4 (0 if it's not related to AI and 4 if it is)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Stagiaire ingénieur en intelligence artificiel...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Stagiaire en développement logiciel Développem...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Stagiaire en développement Web Création et évo...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Stagiaire en développement Web Portage d’une a...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Développeur Data / IA Développement d'applicat...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11281</th>\n",
       "      <td>Opérateur production Montage de transmission a...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11282</th>\n",
       "      <td>Opérateur production Montage de transmission a...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11283</th>\n",
       "      <td>Technicien réparation informatique Reparation ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11284</th>\n",
       "      <td>Technicien réparation Reparation &amp; maintenance...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11286</th>\n",
       "      <td>Développeur web freelance Webdesign &amp; Infographie</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5167 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text  label\n",
       "2      Stagiaire ingénieur en intelligence artificiel...      1\n",
       "3      Stagiaire en développement logiciel Développem...      0\n",
       "4      Stagiaire en développement Web Création et évo...      0\n",
       "5      Stagiaire en développement Web Portage d’une a...      0\n",
       "6      Développeur Data / IA Développement d'applicat...      1\n",
       "...                                                  ...    ...\n",
       "11281  Opérateur production Montage de transmission a...      0\n",
       "11282  Opérateur production Montage de transmission a...      0\n",
       "11283  Technicien réparation informatique Reparation ...      0\n",
       "11284  Technicien réparation Reparation & maintenance...      0\n",
       "11286  Développeur web freelance Webdesign & Infographie      0\n",
       "\n",
       "[5167 rows x 2 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataFrame = pd.read_pickle(r'../data/7587_corrige.pkl')\n",
    "subset = dataFrame[['jobTitle', 'description', 'label']].copy()\n",
    "\n",
    "subset.reset_index(drop=True, inplace=True)\n",
    "subset.replace('', np.nan, inplace=True)\n",
    "subset.dropna(inplace=True)\n",
    "\n",
    "subset['text'] = subset['jobTitle'] + ' ' + subset['description']\n",
    "subset = subset[['text','label']]\n",
    "subset_label_transform = subset.copy()\n",
    "\n",
    "subset_label_transform['label'] = np.where((subset_label_transform[\"label\"] < 3) | (subset_label_transform[\"label\"].isna()), 0, 1)\n",
    "subset_label_transform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split the dataset in two subsets : the training and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utility import split_dataset\n",
    "train_set, test_set = split_dataset(subset_label_transform, 0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Repeat this process with diffent ways of cleaning the data\n",
    "\n",
    "Here instead of considering Nan, 0, 1 and 2 as not being an AI experience and 3 and 4 as being one, we consider :\n",
    "\n",
    "- not AI = 0 and 1 and AI = 3 and 4 (we drop the examples with the label NaN or 2)\n",
    "- not AI = 0 and AI = 4 (we drop the examples with the label NaN, 1, 2 or 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_label_transform_likely_labels = subset.copy()\n",
    "subset_label_transform_likely_labels.replace({2: np.nan}, inplace=True)\n",
    "subset_label_transform_likely_labels.dropna(inplace=True)\n",
    "subset_label_transform_likely_labels['label'] = np.where((subset_label_transform_likely_labels[\"label\"] < 3), 0, 1)\n",
    "\n",
    "subset_label_transform_sure_labels = subset.copy()\n",
    "subset_label_transform_sure_labels.replace({1: np.nan, 2: np.nan, 3: np.nan}, inplace=True)\n",
    "subset_label_transform_sure_labels.dropna(inplace=True)\n",
    "subset_label_transform_sure_labels['label'] = np.where((subset_label_transform_sure_labels[\"label\"] == 0), 0, 1)\n",
    "\n",
    "# We keep the full test set\n",
    "train_set_likely_labels, _ = split_dataset(subset_label_transform_likely_labels, 0.2) \n",
    "train_set_sure_labels, _ = split_dataset(subset_label_transform_sure_labels, 0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\robin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from utility import save_to_json\n",
    "from fewShotBenchmark import n_shot_tests, input_length_tests, distance_tests, loss_tests, language_tests, model_tests, num_epochs_tests, constant_params_tests, data_augmentation_tests, frozen_ratio_tests\n",
    "from models import protonet_f1_score, setfit_f1_score, flair_f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### N-shots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default SetFit uses the oversampling strategy and the Cosine Similarity loss. For instance if we have 8 positive and 8 negative examples then we have:\n",
    "\n",
    "|   | Y | Y | Y | Y | Y | Y | Y | Y | N | N | N | N | N | N | N | N |\n",
    "|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|\n",
    "| Y | + | + | + | + | + | + | + | + | - | - | - | - | - | - | - | - |\n",
    "| Y |   | + | + | + | + | + | + | + | - | - | - | - | - | - | - | - |\n",
    "| Y |   |   | + | + | + | + | + | + | - | - | - | - | - | - | - | - |\n",
    "| Y |   |   |   | + | + | + | + | + | - | - | - | - | - | - | - | - |\n",
    "| Y |   |   |   |   | + | + | + | + | - | - | - | - | - | - | - | - |\n",
    "| Y |   |   |   |   |   | + | + | + | - | - | - | - | - | - | - | - |\n",
    "| Y |   |   |   |   |   |   | + | + | - | - | - | - | - | - | - | - |\n",
    "| Y |   |   |   |   |   |   |   | + | - | - | - | - | - | - | - | - |\n",
    "| N |   |   |   |   |   |   |   |   | + | + | + | + | + | + | + | + |\n",
    "| N |   |   |   |   |   |   |   |   |   | + | + | + | + | + | + | + |\n",
    "| N |   |   |   |   |   |   |   |   |   |   | + | + | + | + | + | + |\n",
    "| N |   |   |   |   |   |   |   |   |   |   |   | + | + | + | + | + |\n",
    "| N |   |   |   |   |   |   |   |   |   |   |   |   | + | + | + | + |\n",
    "| N |   |   |   |   |   |   |   |   |   |   |   |   |   | + | + | + |\n",
    "| N |   |   |   |   |   |   |   |   |   |   |   |   |   |   | + | + |\n",
    "| N |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | + |\n",
    "\n",
    "- P = 2 * (8 + 7 + 6 + 5 + 4 + 3 + 2 + 1) \t= 72\n",
    "- N = 8 * 8 = 64 -> + 8 duplications \t\t= 72\n",
    "- Total = 72 + 72 = 144"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 1 / 4050 Estimated remaining time: ?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "g:\\CYTech\\ING2\\ProjetLinkedin\\2024_02_25\\linkedin-work-experience-classification\\.venv\\lib\\site-packages\\setfit\\data.py:154: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df = df.apply(lambda x: x.sample(min(num_samples, len(x)), random_state=seed))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 2 / 4050 Estimated remaining time: 1262 m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "g:\\CYTech\\ING2\\ProjetLinkedin\\2024_02_25\\linkedin-work-experience-classification\\.venv\\lib\\site-packages\\setfit\\data.py:154: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df = df.apply(lambda x: x.sample(min(num_samples, len(x)), random_state=seed))\n"
     ]
    }
   ],
   "source": [
    "params = {\n",
    "    \"n_shot\": [1,2, 4, 6, 8, 10, 20, 40, 60, 100],\n",
    "    \"n_iter\": 50,\n",
    "    \"n_max_iter_per_shot\": 50,\n",
    "    \"model\": \"sentence-transformers/paraphrase-mpnet-base-v2\",\n",
    "    \"loss\": CosineSimilarityLoss\n",
    "}\n",
    "\n",
    "results, run_times = n_shot_tests(params, train_set, test_set, few_shot_model_f1_function=setfit_f1_score)\n",
    "\n",
    "save_to_json(results, run_times, params,  r'../results/setfit/n_shot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    \"n_shot\": [1,2, 4, 6, 10],\n",
    "    \"n_iter\": 50,\n",
    "    \"n_max_iter_per_shot\": 50,\n",
    "    \"model\": \"sentence-transformers/paraphrase-mpnet-base-v2\",\n",
    "    \"loss\": \"Cosine\"\n",
    "}\n",
    "\n",
    "results, run_times = n_shot_tests(params, train_set, test_set, few_shot_model_f1_function=protonet_f1_score)\n",
    "\n",
    "save_to_json(results, run_times, params,  r'../results/protonet/n_shot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IN PROGRESS (the server from where flair-base can be downloaded is currently down (2024-04-01 19:20))\n",
    "\n",
    "# params = {\n",
    "#     \"n_shot\": [1],\n",
    "#     \"n_iter\": 1,\n",
    "#     \"n_max_iter_per_shot\": 1,\n",
    "#     \"model\": \"flair-base\",\n",
    "#     \"loss\": \"CrossEntropyLoss\"\n",
    "# }\n",
    "\n",
    "# results, run_times = n_shot_tests(params, train_set, test_set, few_shot_model_f1_function=flair_f1_score)\n",
    "\n",
    "# save_to_json(results, run_times, params,  r'../results/flair/n_shot')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    \"input_length_range\": [[0,5],[5,25],[25,50],[50,100],[100,200],[200,350]],\n",
    "    # [[6,10],[10,15],[15,20],[20,30], [6,15], [15,30], [6,20], [10,30], [6,30]],\n",
    "    # [[0,5],[5,10], [10,50], [50,100],[100,200],[200,350]],\n",
    "    # [[0,9],[1,9],[2,9],[3,9],[4,9],[5,9],[6,9],[7,9],[8,9],[9,9]],\n",
    "    # [[0,9],[9,100],[9,350],[100,350],[0,350]],\n",
    "\t# [[8,50],[8,100],[8,150],[8,200],[8,250],[8,300],[8,350]],\n",
    "\t# [[7,350],[8,350],[9,350],[10,350]],\n",
    "    # [[0,3],[0,4],[0,5],[0,6],[0,7],[0,8],[0,9],[0,10]],\n",
    "    # [[0,5],[0,10],[0,100],[6,100],[200,350]],\n",
    "\t\"n_shot\": 10,\n",
    "\t\"n_iter\": 100,\n",
    "\t\"model\": \"sentence-transformers/paraphrase-mpnet-base-v2\",\n",
    "\t\"loss\": CosineSimilarityLoss\n",
    "}\n",
    "\n",
    "results, run_times = input_length_tests(params, train_set, test_set, few_shot_model_f1_function=setfit_f1_score)\n",
    "\n",
    "save_to_json(results, run_times, params,  r'../results/setfit/input_length')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    \"input_length_range\": [[0,5],[5,25],[25,50],[50,100],[100,200],[200,350]],\n",
    "\t\"n_shot\": 10,\n",
    "\t\"n_iter\": 100,\n",
    "\t\"model\": \"sentence-transformers/paraphrase-mpnet-base-v2\",\n",
    "\t\"loss\": \"Cosine\"\n",
    "}\n",
    "\n",
    "results, run_times = input_length_tests(params, train_set, test_set, few_shot_model_f1_function=protonet_f1_score)\n",
    "\n",
    "save_to_json(results, run_times, params,  r'../results/protonet/input_length')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "\t\"n_shot\": 10,\n",
    "\t\"n_iter\": 100,\n",
    "\t\"model\": \"sentence-transformers/paraphrase-mpnet-base-v2\",\n",
    "\t\"distance\": {\n",
    "\t\t\"Cosine\":BatchHardTripletLossDistanceFunction.cosine_distance,\n",
    "\t\t\"Euclidian\": BatchHardTripletLossDistanceFunction.eucledian_distance, # it's really \"eucledian\" and not \"euclidian\" in the module sentence_transformers\n",
    "\t},\n",
    "\t\"loss\": CosineSimilarityLoss,\n",
    "}\n",
    "\n",
    "\n",
    "results, run_times = distance_tests(params, train_set, test_set, few_shot_model_f1_function=setfit_f1_score)\n",
    "\n",
    "save_to_json(results, run_times, params,  r'../results/setfit/distance')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss (pair-wise or Triplet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "\t\"n_shot\": 10,\n",
    "\t\"n_iter\": 100,\n",
    "\t\"model\": \"sentence-transformers/paraphrase-mpnet-base-v2\",\n",
    "\t\"loss\": {\"Pair-wise\":CosineSimilarityLoss, \"Triplet\":BatchAllTripletLoss}\n",
    "}\n",
    "\n",
    "results, run_times = loss_tests(params, train_set, test_set, few_shot_model_f1_function=setfit_f1_score)\n",
    "\n",
    "save_to_json(results, run_times, params,  r'../results/setfit/loss')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "\t\"n_shot\": 10,\n",
    "\t\"lang\": ['fr','en'],\n",
    "\t\"n_iter\": 100,\n",
    "\t\"model\": \"sentence-transformers/paraphrase-mpnet-base-v2\",\n",
    "\t\"loss\": CosineSimilarityLoss\n",
    "}\n",
    "\n",
    "results, run_times = language_tests(params, train_set, test_set, few_shot_model_f1_function=setfit_f1_score)\n",
    "\n",
    "save_to_json(results, run_times, params,  r'../results/setfit/language')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    \"n_shot\": 10,\n",
    "\t\"lang\": ['fr','en'],\n",
    "\t\"n_iter\": 100,\n",
    "\t\"model\": \"sentence-transformers/paraphrase-mpnet-base-v2\",\n",
    "\t\"loss\": \"Cosine\"\n",
    "}\n",
    "\n",
    "results, run_times = language_tests(params, train_set, test_set, few_shot_model_f1_function=protonet_f1_score)\n",
    "\n",
    "save_to_json(results, run_times, params,  r'../results/protonet/language')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "\t\"n_shot\": 10,\n",
    "\t\"n_iter\": 100,\n",
    "\t\"loss\": CosineSimilarityLoss,\n",
    "\t\"model\": {\n",
    "\t\t# \"instructor-large\":\"hkunlp/instructor-large\",\n",
    "\t\t\"GIST-small-Embedding-v0\":\"avsolatorio/GIST-small-Embedding-v0\",\n",
    "\t\t\"gte-tiny\":\"TaylorAI/gte-tiny\",\n",
    "\t\t# \"all-mpnet-base-v2-table\":\"deepset/all-mpnet-base-v2-table\",\n",
    "  \t\t\"paraphrase-mpnet-base-v2\": \"sentence-transformers/paraphrase-mpnet-base-v2\",\n",
    "\t\t# \"all-mpnet-base-v2\":\"sentence-transformers/all-mpnet-base-v2\",\n",
    "\t}\n",
    "}\n",
    "results, run_times = model_tests(params, train_set, test_set, few_shot_model_f1_function=setfit_f1_score)\n",
    "\n",
    "save_to_json(results, run_times, params,  r'../results/setfit/model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    \"n_shot\": 10,\n",
    "\t\"n_iter\": 100,\n",
    "\t\"loss\": \"Cosine\",\n",
    "    \"model\": {\n",
    "        # \"instructor-large\":\"hkunlp/instructor-large\",\n",
    "\t\t\"GIST-small-Embedding-v0\":\"avsolatorio/GIST-small-Embedding-v0\",\n",
    "\t\t\"gte-tiny\":\"TaylorAI/gte-tiny\",\n",
    "\t\t# \"all-mpnet-base-v2-table\":\"deepset/all-mpnet-base-v2-table\",\n",
    "  \t\t\"paraphrase-mpnet-base-v2\": \"sentence-transformers/paraphrase-mpnet-base-v2\",\n",
    "\t\t# \"all-mpnet-base-v2\":\"sentence-transformers/all-mpnet-base-v2\",\n",
    "    }\n",
    "}\n",
    "\n",
    "results, run_times = model_tests(params, train_set, test_set, few_shot_model_f1_function=protonet_f1_score)\n",
    "\n",
    "save_to_json(results, run_times, params,  r'../results/protonet/model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "\t\"n_shot\": 10,\n",
    "\t\"n_iter\": 100,\n",
    "\t\"loss\": CosineSimilarityLoss,\n",
    "\t\"model\": \"sentence-transformers/paraphrase-mpnet-base-v2\",\n",
    "\t\"num_epochs\": [(8,1),(8,2),(8,4),(8,8),(8,10),(8,20),(8,30),(8,40)], \n",
    "\t# [(1,1),(2,1),(4,1),(8,1),(16,1),(32,1),(64,1)], \n",
    "\t# [(1,1),(1,2),(1,4),(1,8),(1,12),(1,16),(1,20),(1,25),(1,30)],\n",
    "}\n",
    "\n",
    "results, run_times = num_epochs_tests(params, train_set, test_set, few_shot_model_f1_function=setfit_f1_score)\n",
    "\n",
    "save_to_json(results, run_times, params,  r'../results/setfit/num_epochs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    \"n_shot\": 10,\n",
    "\t\"n_iter\": 100,\n",
    "\t\"model\": \"sentence-transformers/paraphrase-mpnet-base-v2\",\n",
    "\t\"loss\": \"Cosine\",\n",
    "    \"num_epochs\": [(1,0),(2,0),(4,0),(8,0),(16,0),(32,0)], # There is no classification head \n",
    "}\n",
    "\n",
    "results, run_times = num_epochs_tests(params, train_set, test_set, few_shot_model_f1_function=protonet_f1_score)\n",
    "\n",
    "save_to_json(results, run_times, params,  r'../results/protonet/num_epochs')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data sampling\n",
    "\n",
    "Run multiple tests with different training sets but the same parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# params = {\n",
    "# \t\"n_shot\": 10,\n",
    "# \t\"n_iter\": 100,\n",
    "# \t\"loss\": CosineSimilarityLoss,\n",
    "# \t\"model\": \"sentence-transformers/paraphrase-mpnet-base-v2\",\n",
    "# \t\"input_length_range\":[0,9],\n",
    "# }\n",
    "\n",
    "# results, run_times = constant_params_tests(params, train_set, test_set, few_shot_model_f1_function=setfit_f1_score)\n",
    "\n",
    "# save_to_json(results, run_times, params,  r'../results/setfit/data_sampling')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# params = {\n",
    "# \t\"n_shot\": 10,\n",
    "# \t\"n_iter\": 100,\n",
    "# \t\"loss\": \"Cosine\",\n",
    "# \t\"model\": \"sentence-transformers/paraphrase-mpnet-base-v2\",\n",
    "# \t\"input_length_range\":[0,9],\n",
    "#     \"ratio_frozen_weights\": 0.5\n",
    "# }\n",
    "\n",
    "# results, run_times = constant_params_tests(params, train_set, test_set, few_shot_model_f1_function=setfit_f1_score)\n",
    "\n",
    "# save_to_json(results, run_times, params,  r'../results/protonet/data_sampling')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Augmentation\n",
    "\n",
    "For now we only use a back translation technique and synonym replacement, but we could try other ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    \"n_shot\": 10,\n",
    "    \"n_iter\": 100,\n",
    "    \"loss\": CosineSimilarityLoss,\n",
    "    \"model\": \"sentence-transformers/paraphrase-mpnet-base-v2\",\n",
    "    \"data_augmentation_ratio\": 1.3, # + 30 %\n",
    "    \"data_augmentation_strategy\":[\"none\",\"swapping_inter\", \"back_translation\", \"synonym_replacement\", \"crossover\"],\n",
    "    \"strategy_params\": {\n",
    "        \"n_points_crossover\": 2,\n",
    "        \"modification_rate\": 0.5,\n",
    "    }\n",
    "}\n",
    "\n",
    "results, run_times = data_augmentation_tests(params, train_set, test_set, few_shot_model_f1_function=setfit_f1_score)\n",
    "save_to_json(results, run_times, params,  r'../results/setfit/data_augmentation')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Frozen weights ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    \"n_shot\": 10,\n",
    "\t\"n_iter\": 100,\n",
    "\t\"model\": \"sentence-transformers/paraphrase-mpnet-base-v2\",\n",
    "\t\"loss\": \"Cosine\",\n",
    "    \"ratio_frozen_weights\": [0.1,0.3,0.5,0.7,0.9]\n",
    "}\n",
    "\n",
    "results, run_times = frozen_ratio_tests(params, train_set, test_set, few_shot_model_f1_function=protonet_f1_score)\n",
    "\n",
    "save_to_json(results, run_times, params,  r'../results/protonet/frozen_ratio')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset label selection\n",
    "\n",
    "Only select labels 0,1,3,4 and then 0,1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "\t\"n_shot\": 10,\n",
    "\t\"n_iter\": 100,\n",
    "\t\"loss\": CosineSimilarityLoss,\n",
    "\t\"model\": \"sentence-transformers/paraphrase-mpnet-base-v2\",\n",
    "}\n",
    "\n",
    "tested_training_sets = {\n",
    "\t\"all_labels\": train_set,\n",
    "\t\"likely_labels\":train_set_likely_labels,\n",
    "\t\"sure_labels\":train_set_sure_labels,\n",
    "}\n",
    "\n",
    "results = {}\n",
    "run_times = {}\n",
    "progress = 0\n",
    "progress_end = len(tested_training_sets)\n",
    "\n",
    "for training_set_key, training_set_data in tested_training_sets.items():\n",
    "\tprint(\"Test: \", progress,\"/\",progress_end)\n",
    "\ttemp_results, temp_run_times = constant_params_tests(params, training_set_data, test_set)\n",
    "\tresults[training_set_key] = temp_results[\"all\"]\n",
    "\trun_times[training_set_key] = temp_run_times[\"all\"]\n",
    "\n",
    "params[\"training_set\"] = list(tested_training_sets.keys())\n",
    "save_to_json(results, run_times, params,  r'../results/setfit/training_set_labels_restriction')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utility import load_all_results_data, create_boxplot, create_bar_plot, load_latest_results_data, create_scatter_line_plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot latest graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results, run_times, _ = load_latest_results_data(r'../results/flair/n_shot')\n",
    "# create_boxplot(results, 'Effect of the number of shots on F1-score', 'N-shot', 'F1-score',y_min=0, y_max=1)\n",
    "# create_bar_plot(run_times, 'Runtime for the N-shot tests', 'N-shot', 'Run time (s)', y_min=0, y_max=1000)\n",
    "\n",
    "# results, run_times, _ = load_latest_results_data(r'../results/setfit/input_length')\n",
    "# create_boxplot(results, 'Effect of the length of the input on F1-score (train set only) (8 shots)', 'Number of words', 'F1-score')\n",
    "# create_bar_plot(run_times, 'Runtime for the input length tests', 'Number of words', 'Run time (s)')\n",
    "\n",
    "# results, run_times, _ = load_latest_results_data(r'../results/setfit/distance')\n",
    "# create_boxplot(results, 'Effect of the distance on F1-score', 'Distance', 'F1-score')\n",
    "# create_bar_plot(run_times, 'Runtime for the distance tests', 'Distance', 'Run time (s)')\n",
    "\n",
    "# results, run_times, _ = load_latest_results_data(r'../results/setfit/loss')\n",
    "# create_boxplot(results, 'Effect of the loss on F1-score', 'Loss', 'F1-score')\n",
    "# create_bar_plot(run_times, 'Runtime for the loss tests', 'Loss', 'Run time (s)')\n",
    "\n",
    "# results, run_times, _ = load_latest_results_data(r'../results/setfit/language')\n",
    "# create_boxplot(results, 'Effect of the language on F1-score (train and test sets)', 'Language', 'F1-score')\n",
    "# create_bar_plot(run_times, 'Runtime for the language tests', 'Language', 'Run time (s)')\n",
    "\n",
    "# results, run_times, _ = load_latest_results_data(r'../results/setfit/model')\n",
    "# create_boxplot(results, 'Effect of the Sentence Transformer model on F1-score', 'Model', 'F1-score', vertical_xticks=True)\n",
    "# create_bar_plot(run_times, 'Runtime for the Sentence Transformer model tests', 'Model', 'Run time (s)', vertical_xticks=True)\n",
    "\n",
    "# results, run_times, _ = load_latest_results_data(r'../results/setfit/num_epochs')\n",
    "# create_boxplot(results, 'Effect of the number of epochs on F1-score', 'Number of epochs', 'F1-score')\n",
    "# create_bar_plot(run_times, 'Runtime for the number of epochs tests', 'Number of epochs', 'Run time (s)')\n",
    "\n",
    "# results, run_times, _ = load_latest_results_data(r'../results/setfit/head_learning_rate')\n",
    "# create_boxplot(results, 'Effect of the head learning rate on F1-score', 'Head learning rate', 'F1-score')\n",
    "# create_bar_plot(run_times, 'Runtime for the head learning rate tests', 'Head learning rate', 'Run time (s)')\n",
    "\n",
    "# results, run_times, params = load_latest_results_data(r'../results/setfit/data_sampling')\n",
    "# create_boxplot(results, f\"F1-score with the same params with {params['n_iter']} different support sets\", str(params), 'F1-score')\n",
    "# create_bar_plot(run_times, 'Runtime for the same params', str(params), 'Run time (s)')\n",
    "\n",
    "# results, run_times, params = load_latest_results_data(r'../results/setfit/data_augmentation')\n",
    "# create_boxplot(results, \"Effect of data augmentation on F1-score\", \"Data augmentation strategy\", 'F1-score', vertical_xticks=True)\n",
    "\n",
    "# results, run_times, params = load_latest_results_data(r'../results/protonet/n_shot')\n",
    "# create_boxplot(results, 'Effect of the number of shots on F1-score', 'N-shot', 'F1-score')\n",
    "# create_bar_plot(run_times, 'Runtime for the N-shot tests', 'N-shot', 'Run time (s)')\n",
    "\n",
    "# results, run_times, _ = load_latest_results_data(r'../results/protonet/input_length')\n",
    "# create_boxplot(results, 'Effect of the length of the input on F1-score (train set only)', 'Number of words', 'F1-score')\n",
    "# create_bar_plot(run_times, 'Runtime for the input length tests', 'Number of words', 'Run time (s)')\n",
    "\n",
    "# results, run_times, _ = load_latest_results_data(r'../results/protonet/language')\n",
    "# create_boxplot(results, 'Effect of the language on F1-score (train and test sets)', 'Language', 'F1-score')\n",
    "# create_bar_plot(run_times, 'Runtime for the language tests', 'Language', 'Run time (s)')\n",
    "\n",
    "# results, run_times, _ = load_latest_results_data(r'../results/protonet/frozen_ratio')\n",
    "# create_boxplot(results, 'Effect of the frozen weights ratio for the ProtoNet on F1-score (train and test sets)', 'Frozen weights ratio', 'F1-score')\n",
    "# create_bar_plot(run_times, 'Runtime for the frozen weights ratio tests', 'Frozen weights ratio', 'Run time (s)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot graphs using all data and filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results, run_times = load_all_results_data(r'../results/setfit/n_shot', \"n_shot\")\n",
    "# create_boxplot(results, 'Effect of the number of shots on F1-score', 'N-shot', 'F1-score')\n",
    "# create_bar_plot(run_times, 'Runtime for the N-shot tests', 'N-shot', 'Run time (s)')\n",
    "\n",
    "# results, run_times = load_all_results_data(r'../results/setfit/input_length', \"input_length_range\", {\"input_length_range\":[[0,5],[5,25],[25,50],[50,100],[100,200],[200,350]]})\n",
    "# create_boxplot(results, 'Effect of the length of the input on F1-score (train set only)', 'Number of words', 'F1-score')\n",
    "# create_bar_plot(run_times, 'Runtime for the N-shot tests', 'N-shot', 'Run time (s)')\n",
    "\n",
    "# results, run_times = load_all_results_data(r'../results/setfit/language', \"lang\", {\"model\":\"sentence-transformers/paraphrase-multilingual-mpnet-base-v2\", \"n_shot\":8})\n",
    "# create_boxplot(results, 'Effect of the language on F1-score (train and test sets) (multilingual model) (8 shots)', 'Language', 'F1-score')\n",
    "\n",
    "# results, run_times = load_all_results_data(r'../results/setfit/language', \"lang\", {\"model\":\"sentence-transformers/paraphrase-mpnet-base-v2\", \"n_shot\":8})\n",
    "# create_boxplot(results, 'Effect of the language on F1-score (train and test sets) (not multilingual model) (8 shots)', 'Language', 'F1-score')\n",
    "\n",
    "# results, run_times = load_all_results_data(r'../results/setfit/model', \"model\", {\"n_shot\":8})\n",
    "# create_boxplot(results, 'Effect of the model on F1-score (8 shots)', 'Model', 'F1-score', vertical_xticks=True)\n",
    "\n",
    "# results, run_times = load_all_results_data(r'../results/setfit/num_epochs', \"num_epochs\", {\"n_shot\":8, \"num_epochs\":[(1,1),(2,1),(4,1),(8,1),(16,1),(32,1),(64,1)]})\n",
    "# create_boxplot(results, 'Effect of the number of epochs (body) on F1-score (8 shots)', 'Number of epochs', 'F1-score', custom_xticks=['1','2','4','8','16','32','64'])\n",
    "# create_bar_plot(run_times, 'Runtime for the body epochs tests', 'Number of epochs', 'Run time (s)', custom_xticks=['1','2','4','8','16','32','64'])\n",
    "\n",
    "# results, run_times = load_all_results_data(r'../results/setfit/data_augmentation', \"data_augmentation_strategy\", {\"data_augmentation_strategy\":[\"none\", \"back_translation\",\"crossover\",\"swapping_inter\",\"synonym_replacement\"]})\n",
    "# create_boxplot(results, f\"Effect of the data augmentation ratio on F1-score (8 shots)\", 'Strategy', 'F1-score', vertical_xticks=True)\n",
    "# create_bar_plot(results, f\"Runtime for the data augmentation tests\", 'Strategy', 'Run time (s)', vertical_xticks=True)\n",
    "\n",
    "# results, run_times = load_all_results_data(r'../results/setfit/training_set_labels_restriction', \"model\")\n",
    "# create_boxplot(results, 'Effect of the training set labels restriction on F1-score', 'Model', 'F1-score', vertical_xticks=True)\n",
    "\n",
    "# results, run_times = load_all_results_data(r'../results/protonet/n_shot', \"n_shot\")\n",
    "# create_boxplot(results, 'Effect of the number of shots on F1-score', 'N-shot', 'F1-score')\n",
    "# create_bar_plot(run_times, 'Runtime for the N-shot tests', 'N-shot', 'Run time (s)')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
