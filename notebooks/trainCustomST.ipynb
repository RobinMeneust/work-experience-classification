{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define functions and parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This script is from https://github.com/UKPLab/sentence-transformers/tree/master/\n",
    "Paper authors: Reimers, Nils and Gurevych, Iryna\n",
    "\"\"\"\n",
    "\n",
    "from sentence_transformers import SentenceTransformer, LoggingHandler, models, evaluation, losses\n",
    "from torch.utils.data import DataLoader\n",
    "from sentence_transformers.datasets import ParallelSentencesDataset\n",
    "from datetime import datetime\n",
    "\n",
    "import os\n",
    "import logging\n",
    "import sentence_transformers.util\n",
    "import csv\n",
    "import gzip\n",
    "from tqdm.autonotebook import tqdm\n",
    "import numpy as np\n",
    "import zipfile\n",
    "import io\n",
    "\n",
    "logging.basicConfig(\n",
    "    format=\"%(asctime)s - %(message)s\", datefmt=\"%Y-%m-%d %H:%M:%S\", level=logging.INFO, handlers=[LoggingHandler()]\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "teacher_model_name = (\n",
    "    \"sentence-transformers/paraphrase-mpnet-base-v2\"  # Our monolingual teacher model, we want to convert to multiple languages\n",
    ")\n",
    "student_model_name = \"RobinMeneust/paraphrase-mpnet-base-v2-fr-en\"  # Model we use to imitate the teacher model\n",
    "\n",
    "max_seq_length = 128  # Student model max. lengths for inputs (number of word pieces)\n",
    "train_batch_size = 64  # Batch size for training\n",
    "inference_batch_size = 64  # Batch size at inference\n",
    "max_sentences_per_language = 500000  # Maximum number of  parallel sentences for training\n",
    "train_max_sentence_length = 250  # Maximum length (characters) for parallel training sentences\n",
    "\n",
    "num_epochs = 5  # Train for x epochs\n",
    "num_warmup_steps = 10000  # Warumup steps\n",
    "\n",
    "num_evaluation_steps = 1000  # Evaluate performance after every xxxx steps\n",
    "dev_sentences = 1000  # Number of parallel sentences to be used for development\n",
    "\n",
    "\n",
    "# Define the language codes you would like to extend the model to\n",
    "source_languages = set([\"en\"])  # Our teacher model accepts English (en) sentences\n",
    "target_languages = set([\"fr\"])  # We want to extend the model to these new languages. For language codes, see the header of the train file\n",
    "\n",
    "\n",
    "output_path = (\n",
    "    \"output/make-multilingual-\"\n",
    "    + \"-\".join(sorted(list(source_languages)) + sorted(list(target_languages)))\n",
    "    + \"-\"\n",
    "    + datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    ")\n",
    "\n",
    "\n",
    "# This function downloads a corpus if it does not exist\n",
    "def download_corpora(filepaths):\n",
    "    if not isinstance(filepaths, list):\n",
    "        filepaths = [filepaths]\n",
    "\n",
    "    for filepath in filepaths:\n",
    "        if not os.path.exists(filepath):\n",
    "            print(filepath, \"does not exists. Try to download from server\")\n",
    "            filename = os.path.basename(filepath)\n",
    "            url = \"https://sbert.net/datasets/\" + filename\n",
    "            sentence_transformers.util.http_get(url, filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we define train train and dev corpora\n",
    "train_corpus = \"datasets/parallel-sentences.tsv.gz\"\n",
    "sts_corpus = \"datasets/stsbenchmark.zip\"\n",
    "parallel_sentences_folder = \"parallel-sentences/\"\n",
    "\n",
    "print(\"Check if the file exists. If not, they are downloaded...\")\n",
    "# Check if the file exists. If not, they are downloaded\n",
    "download_corpora([train_corpus, sts_corpus])\n",
    "\n",
    "print(\"Create parallel files for the selected language combinations...\")\n",
    "# Create parallel files for the selected language combinations\n",
    "os.makedirs(parallel_sentences_folder, exist_ok=True)\n",
    "train_files = []\n",
    "dev_files = []\n",
    "files_to_create = []\n",
    "for source_lang in source_languages:\n",
    "    for target_lang in target_languages:\n",
    "        output_filename_train = os.path.join(\n",
    "            parallel_sentences_folder, \"talks-{}-{}-train.tsv.gz\".format(source_lang, target_lang)\n",
    "        )\n",
    "        output_filename_dev = os.path.join(\n",
    "            parallel_sentences_folder, \"talks-{}-{}-dev.tsv.gz\".format(source_lang, target_lang)\n",
    "        )\n",
    "        train_files.append(output_filename_train)\n",
    "        dev_files.append(output_filename_dev)\n",
    "        if not os.path.exists(output_filename_train) or not os.path.exists(output_filename_dev):\n",
    "            files_to_create.append(\n",
    "                {\n",
    "                    \"src_lang\": source_lang,\n",
    "                    \"trg_lang\": target_lang,\n",
    "                    \"fTrain\": gzip.open(output_filename_train, \"wt\", encoding=\"utf8\"),\n",
    "                    \"fDev\": gzip.open(output_filename_dev, \"wt\", encoding=\"utf8\"),\n",
    "                    \"devCount\": 0,\n",
    "                }\n",
    "            )\n",
    "\n",
    "if len(files_to_create) > 0:\n",
    "    print(\n",
    "        \"Parallel sentences files {} do not exist. Create these files now\".format(\n",
    "            \", \".join(map(lambda x: x[\"src_lang\"] + \"-\" + x[\"trg_lang\"], files_to_create))\n",
    "        )\n",
    "    )\n",
    "    with gzip.open(train_corpus, \"rt\", encoding=\"utf8\") as fIn:\n",
    "        reader = csv.DictReader(fIn, delimiter=\"\\t\", quoting=csv.QUOTE_NONE)\n",
    "        for line in tqdm(reader, desc=\"Sentences\"):\n",
    "            for outfile in files_to_create:\n",
    "                src_text = line[outfile[\"src_lang\"]].strip()\n",
    "                trg_text = line[outfile[\"trg_lang\"]].strip()\n",
    "\n",
    "                if src_text != \"\" and trg_text != \"\":\n",
    "                    if outfile[\"devCount\"] < dev_sentences:\n",
    "                        outfile[\"devCount\"] += 1\n",
    "                        fOut = outfile[\"fDev\"]\n",
    "                    else:\n",
    "                        fOut = outfile[\"fTrain\"]\n",
    "\n",
    "                    fOut.write(\"{}\\t{}\\n\".format(src_text, trg_text))\n",
    "\n",
    "    for outfile in files_to_create:\n",
    "        outfile[\"fTrain\"].close()\n",
    "        outfile[\"fDev\"].close()\n",
    "\n",
    "\n",
    "######## Start the extension of the teacher model to multiple languages ########\n",
    "logger.info(\"Load teacher model\")\n",
    "teacher_model = SentenceTransformer(teacher_model_name)\n",
    "\n",
    "\n",
    "logger.info(\"Create student model from scratch\")\n",
    "word_embedding_model = models.Transformer(student_model_name, max_seq_length=max_seq_length)\n",
    "# Apply mean pooling to get one fixed sized sentence vector\n",
    "pooling_model = models.Pooling(word_embedding_model.get_word_embedding_dimension())\n",
    "student_model = SentenceTransformer(modules=[word_embedding_model, pooling_model])\n",
    "\n",
    "print(\"Read Parallel Sentences Dataset...\")\n",
    "###### Read Parallel Sentences Dataset ######\n",
    "train_data = ParallelSentencesDataset(\n",
    "    student_model=student_model, teacher_model=teacher_model, batch_size=inference_batch_size, use_embedding_cache=True\n",
    ")\n",
    "for train_file in train_files:\n",
    "    train_data.load_data(\n",
    "        train_file, max_sentences=max_sentences_per_language, max_sentence_length=train_max_sentence_length\n",
    "    )\n",
    "\n",
    "train_dataloader = DataLoader(train_data, shuffle=True, batch_size=train_batch_size)\n",
    "train_loss = losses.MSELoss(model=student_model)\n",
    "\n",
    "print(\"Evaluate cross-lingual performance on different tasks...\")\n",
    "#### Evaluate cross-lingual performance on different tasks #####\n",
    "evaluators = []  # evaluators has a list of different evaluator classes we call periodically\n",
    "\n",
    "for dev_file in dev_files:\n",
    "    logger.info(\"Create evaluator for \" + dev_file)\n",
    "    src_sentences = []\n",
    "    trg_sentences = []\n",
    "    with gzip.open(dev_file, \"rt\", encoding=\"utf8\") as fIn:\n",
    "        for line in fIn:\n",
    "            splits = line.strip().split(\"\\t\")\n",
    "            if splits[0] != \"\" and splits[1] != \"\":\n",
    "                src_sentences.append(splits[0])\n",
    "                trg_sentences.append(splits[1])\n",
    "\n",
    "    # Mean Squared Error (MSE) measures the (euclidean) distance between teacher and student embeddings\n",
    "    dev_mse = evaluation.MSEEvaluator(\n",
    "        src_sentences,\n",
    "        trg_sentences,\n",
    "        name=os.path.basename(dev_file),\n",
    "        teacher_model=teacher_model,\n",
    "        batch_size=inference_batch_size,\n",
    "    )\n",
    "    evaluators.append(dev_mse)\n",
    "\n",
    "    # TranslationEvaluator computes the embeddings for all parallel sentences. It then check if the embedding of source[i] is the closest to target[i] out of all available target sentences\n",
    "    dev_trans_acc = evaluation.TranslationEvaluator(\n",
    "        src_sentences, trg_sentences, name=os.path.basename(dev_file), batch_size=inference_batch_size\n",
    "    )\n",
    "    evaluators.append(dev_trans_acc)\n",
    "\n",
    "print(\"Read cross-lingual Semantic Textual Similarity (STS) data...\")\n",
    "##### Read cross-lingual Semantic Textual Similarity (STS) data ####\n",
    "all_languages = list(set(list(source_languages) + list(target_languages)))\n",
    "sts_data = {}\n",
    "\n",
    "# Open the ZIP File of STS2017-extended.zip and check for which language combinations we have STS data\n",
    "with zipfile.ZipFile(sts_corpus) as zip:\n",
    "    filelist = zip.namelist()\n",
    "    sts_files = []\n",
    "\n",
    "    for i in range(len(all_languages)):\n",
    "        for j in range(i, len(all_languages)):\n",
    "            lang1 = all_languages[i]\n",
    "            lang2 = all_languages[j]\n",
    "            filepath = \"STS2017-extended/STS.{}-{}.txt\".format(lang1, lang2)\n",
    "            if filepath not in filelist:\n",
    "                lang1, lang2 = lang2, lang1\n",
    "                filepath = \"STS2017-extended/STS.{}-{}.txt\".format(lang1, lang2)\n",
    "\n",
    "            if filepath in filelist:\n",
    "                filename = os.path.basename(filepath)\n",
    "                sts_data[filename] = {\"sentences1\": [], \"sentences2\": [], \"scores\": []}\n",
    "\n",
    "                fIn = zip.open(filepath)\n",
    "                for line in io.TextIOWrapper(fIn, \"utf8\"):\n",
    "                    sent1, sent2, score = line.strip().split(\"\\t\")\n",
    "                    score = float(score)\n",
    "                    sts_data[filename][\"sentences1\"].append(sent1)\n",
    "                    sts_data[filename][\"sentences2\"].append(sent2)\n",
    "                    sts_data[filename][\"scores\"].append(score)\n",
    "\n",
    "for filename, data in sts_data.items():\n",
    "    test_evaluator = evaluation.EmbeddingSimilarityEvaluator(\n",
    "        data[\"sentences1\"],\n",
    "        data[\"sentences2\"],\n",
    "        data[\"scores\"],\n",
    "        batch_size=inference_batch_size,\n",
    "        name=filename,\n",
    "        show_progress_bar=False,\n",
    "    )\n",
    "    evaluators.append(test_evaluator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "\n",
    "num_epochs = 1\n",
    "\n",
    "student_model.fit(\n",
    "    train_objectives=[(train_dataloader, train_loss)],\n",
    "    evaluator=evaluation.SequentialEvaluator(evaluators, main_score_function=lambda scores: np.mean(scores)),\n",
    "    epochs=num_epochs,\n",
    "    warmup_steps=num_warmup_steps,\n",
    "    evaluation_steps=num_evaluation_steps,\n",
    "    output_path=output_path,\n",
    "    save_best_model=True,\n",
    "    optimizer_params={\"lr\": 2e-5, \"eps\": 1e-6},\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from huggingface_hub import login\n",
    "\n",
    "login()\n",
    "# Push to the Hub\n",
    "student_model.save_to_hub(\"paraphrase-mpnet-base-v2-fr-en\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
