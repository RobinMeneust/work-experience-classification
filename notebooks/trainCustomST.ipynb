{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define functions and parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This script is from https://github.com/UKPLab/sentence-transformers/tree/master/\n",
    "Paper authors: Reimers, Nils and Gurevych, Iryna\n",
    "\"\"\"\n",
    "\n",
    "from sentence_transformers import SentenceTransformer, LoggingHandler, models, evaluation, losses\n",
    "from torch.utils.data import DataLoader\n",
    "from sentence_transformers.datasets import ParallelSentencesDataset\n",
    "from datetime import datetime\n",
    "\n",
    "import os\n",
    "import logging\n",
    "import sentence_transformers.util\n",
    "import csv\n",
    "import gzip\n",
    "from tqdm.autonotebook import tqdm\n",
    "import numpy as np\n",
    "import zipfile\n",
    "import io\n",
    "\n",
    "logging.basicConfig(\n",
    "    format=\"%(asctime)s - %(message)s\", datefmt=\"%Y-%m-%d %H:%M:%S\", level=logging.INFO, handlers=[LoggingHandler()]\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "teacher_model_name = (\n",
    "    \"sentence-transformers/paraphrase-mpnet-base-v2\"  # Our monolingual teacher model, we want to convert to multiple languages\n",
    ")\n",
    "student_model_name = \"RobinMeneust/paraphrase-mpnet-base-v2-fr-en\"  # Model we use to imitate the teacher model\n",
    "\n",
    "max_seq_length = 128  # Student model max. lengths for inputs (number of word pieces)\n",
    "train_batch_size = 64  # Batch size for training\n",
    "inference_batch_size = 64  # Batch size at inference\n",
    "max_sentences_per_language = 500000  # Maximum number of  parallel sentences for training\n",
    "train_max_sentence_length = 250  # Maximum length (characters) for parallel training sentences\n",
    "\n",
    "num_epochs = 5  # Train for x epochs\n",
    "num_warmup_steps = 10000  # Warumup steps\n",
    "\n",
    "num_evaluation_steps = 1000  # Evaluate performance after every xxxx steps\n",
    "dev_sentences = 1000  # Number of parallel sentences to be used for development\n",
    "\n",
    "\n",
    "# Define the language codes you would like to extend the model to\n",
    "source_languages = set([\"en\"])  # Our teacher model accepts English (en) sentences\n",
    "target_languages = set([\"fr\"])  # We want to extend the model to these new languages. For language codes, see the header of the train file\n",
    "\n",
    "\n",
    "output_path = (\n",
    "    \"output/make-multilingual-\"\n",
    "    + \"-\".join(sorted(list(source_languages)) + sorted(list(target_languages)))\n",
    "    + \"-\"\n",
    "    + datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    ")\n",
    "\n",
    "\n",
    "# This function downloads a corpus if it does not exist\n",
    "def download_corpora(filepaths):\n",
    "    if not isinstance(filepaths, list):\n",
    "        filepaths = [filepaths]\n",
    "\n",
    "    for filepath in filepaths:\n",
    "        if not os.path.exists(filepath):\n",
    "            print(filepath, \"does not exists. Try to download from server\")\n",
    "            filename = os.path.basename(filepath)\n",
    "            url = \"https://sbert.net/datasets/\" + filename\n",
    "            sentence_transformers.util.http_get(url, filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check if the file exists. If not, they are downloaded...\n",
      "Create parallel files for the selected language combinations...\n",
      "2024-03-14 20:25:59 - Load teacher model\n",
      "2024-03-14 20:25:59 - Load pretrained SentenceTransformer: sentence-transformers/paraphrase-mpnet-base-v2\n",
      "2024-03-14 20:26:01 - Use pytorch device_name: cuda\n",
      "2024-03-14 20:26:01 - Create student model from scratch\n",
      "2024-03-14 20:26:01 - Use pytorch device_name: cuda\n",
      "Read Parallel Sentences Dataset...\n",
      "2024-03-14 20:26:01 - Load parallel-sentences/talks-en-fr-train.tsv.gz\n",
      "Evaluate cross-lingual performance on different tasks...\n",
      "2024-03-14 20:26:03 - Create evaluator for parallel-sentences/talks-en-fr-dev.tsv.gz\n",
      "Read cross-lingual Semantic Textual Similarity (STS) data...\n"
     ]
    }
   ],
   "source": [
    "# Here we define train train and dev corpora\n",
    "train_corpus = \"datasets/parallel-sentences.tsv.gz\"\n",
    "sts_corpus = \"datasets/stsbenchmark.zip\"\n",
    "parallel_sentences_folder = \"parallel-sentences/\"\n",
    "\n",
    "print(\"Check if the file exists. If not, they are downloaded...\")\n",
    "# Check if the file exists. If not, they are downloaded\n",
    "download_corpora([train_corpus, sts_corpus])\n",
    "\n",
    "print(\"Create parallel files for the selected language combinations...\")\n",
    "# Create parallel files for the selected language combinations\n",
    "os.makedirs(parallel_sentences_folder, exist_ok=True)\n",
    "train_files = []\n",
    "dev_files = []\n",
    "files_to_create = []\n",
    "for source_lang in source_languages:\n",
    "    for target_lang in target_languages:\n",
    "        output_filename_train = os.path.join(\n",
    "            parallel_sentences_folder, \"talks-{}-{}-train.tsv.gz\".format(source_lang, target_lang)\n",
    "        )\n",
    "        output_filename_dev = os.path.join(\n",
    "            parallel_sentences_folder, \"talks-{}-{}-dev.tsv.gz\".format(source_lang, target_lang)\n",
    "        )\n",
    "        train_files.append(output_filename_train)\n",
    "        dev_files.append(output_filename_dev)\n",
    "        if not os.path.exists(output_filename_train) or not os.path.exists(output_filename_dev):\n",
    "            files_to_create.append(\n",
    "                {\n",
    "                    \"src_lang\": source_lang,\n",
    "                    \"trg_lang\": target_lang,\n",
    "                    \"fTrain\": gzip.open(output_filename_train, \"wt\", encoding=\"utf8\"),\n",
    "                    \"fDev\": gzip.open(output_filename_dev, \"wt\", encoding=\"utf8\"),\n",
    "                    \"devCount\": 0,\n",
    "                }\n",
    "            )\n",
    "\n",
    "if len(files_to_create) > 0:\n",
    "    print(\n",
    "        \"Parallel sentences files {} do not exist. Create these files now\".format(\n",
    "            \", \".join(map(lambda x: x[\"src_lang\"] + \"-\" + x[\"trg_lang\"], files_to_create))\n",
    "        )\n",
    "    )\n",
    "    with gzip.open(train_corpus, \"rt\", encoding=\"utf8\") as fIn:\n",
    "        reader = csv.DictReader(fIn, delimiter=\"\\t\", quoting=csv.QUOTE_NONE)\n",
    "        for line in tqdm(reader, desc=\"Sentences\"):\n",
    "            for outfile in files_to_create:\n",
    "                src_text = line[outfile[\"src_lang\"]].strip()\n",
    "                trg_text = line[outfile[\"trg_lang\"]].strip()\n",
    "\n",
    "                if src_text != \"\" and trg_text != \"\":\n",
    "                    if outfile[\"devCount\"] < dev_sentences:\n",
    "                        outfile[\"devCount\"] += 1\n",
    "                        fOut = outfile[\"fDev\"]\n",
    "                    else:\n",
    "                        fOut = outfile[\"fTrain\"]\n",
    "\n",
    "                    fOut.write(\"{}\\t{}\\n\".format(src_text, trg_text))\n",
    "\n",
    "    for outfile in files_to_create:\n",
    "        outfile[\"fTrain\"].close()\n",
    "        outfile[\"fDev\"].close()\n",
    "\n",
    "\n",
    "######## Start the extension of the teacher model to multiple languages ########\n",
    "logger.info(\"Load teacher model\")\n",
    "teacher_model = SentenceTransformer(teacher_model_name)\n",
    "\n",
    "\n",
    "logger.info(\"Create student model from scratch\")\n",
    "word_embedding_model = models.Transformer(student_model_name, max_seq_length=max_seq_length)\n",
    "# Apply mean pooling to get one fixed sized sentence vector\n",
    "pooling_model = models.Pooling(word_embedding_model.get_word_embedding_dimension())\n",
    "student_model = SentenceTransformer(modules=[word_embedding_model, pooling_model])\n",
    "\n",
    "print(\"Read Parallel Sentences Dataset...\")\n",
    "###### Read Parallel Sentences Dataset ######\n",
    "train_data = ParallelSentencesDataset(\n",
    "    student_model=student_model, teacher_model=teacher_model, batch_size=inference_batch_size, use_embedding_cache=True\n",
    ")\n",
    "for train_file in train_files:\n",
    "    train_data.load_data(\n",
    "        train_file, max_sentences=max_sentences_per_language, max_sentence_length=train_max_sentence_length\n",
    "    )\n",
    "\n",
    "train_dataloader = DataLoader(train_data, shuffle=True, batch_size=train_batch_size)\n",
    "train_loss = losses.MSELoss(model=student_model)\n",
    "\n",
    "print(\"Evaluate cross-lingual performance on different tasks...\")\n",
    "#### Evaluate cross-lingual performance on different tasks #####\n",
    "evaluators = []  # evaluators has a list of different evaluator classes we call periodically\n",
    "\n",
    "for dev_file in dev_files:\n",
    "    logger.info(\"Create evaluator for \" + dev_file)\n",
    "    src_sentences = []\n",
    "    trg_sentences = []\n",
    "    with gzip.open(dev_file, \"rt\", encoding=\"utf8\") as fIn:\n",
    "        for line in fIn:\n",
    "            splits = line.strip().split(\"\\t\")\n",
    "            if splits[0] != \"\" and splits[1] != \"\":\n",
    "                src_sentences.append(splits[0])\n",
    "                trg_sentences.append(splits[1])\n",
    "\n",
    "    # Mean Squared Error (MSE) measures the (euclidean) distance between teacher and student embeddings\n",
    "    dev_mse = evaluation.MSEEvaluator(\n",
    "        src_sentences,\n",
    "        trg_sentences,\n",
    "        name=os.path.basename(dev_file),\n",
    "        teacher_model=teacher_model,\n",
    "        batch_size=inference_batch_size,\n",
    "    )\n",
    "    evaluators.append(dev_mse)\n",
    "\n",
    "    # TranslationEvaluator computes the embeddings for all parallel sentences. It then check if the embedding of source[i] is the closest to target[i] out of all available target sentences\n",
    "    dev_trans_acc = evaluation.TranslationEvaluator(\n",
    "        src_sentences, trg_sentences, name=os.path.basename(dev_file), batch_size=inference_batch_size\n",
    "    )\n",
    "    evaluators.append(dev_trans_acc)\n",
    "\n",
    "print(\"Read cross-lingual Semantic Textual Similarity (STS) data...\")\n",
    "##### Read cross-lingual Semantic Textual Similarity (STS) data ####\n",
    "all_languages = list(set(list(source_languages) + list(target_languages)))\n",
    "sts_data = {}\n",
    "\n",
    "# Open the ZIP File of STS2017-extended.zip and check for which language combinations we have STS data\n",
    "with zipfile.ZipFile(sts_corpus) as zip:\n",
    "    filelist = zip.namelist()\n",
    "    sts_files = []\n",
    "\n",
    "    for i in range(len(all_languages)):\n",
    "        for j in range(i, len(all_languages)):\n",
    "            lang1 = all_languages[i]\n",
    "            lang2 = all_languages[j]\n",
    "            filepath = \"STS2017-extended/STS.{}-{}.txt\".format(lang1, lang2)\n",
    "            if filepath not in filelist:\n",
    "                lang1, lang2 = lang2, lang1\n",
    "                filepath = \"STS2017-extended/STS.{}-{}.txt\".format(lang1, lang2)\n",
    "\n",
    "            if filepath in filelist:\n",
    "                filename = os.path.basename(filepath)\n",
    "                sts_data[filename] = {\"sentences1\": [], \"sentences2\": [], \"scores\": []}\n",
    "\n",
    "                fIn = zip.open(filepath)\n",
    "                for line in io.TextIOWrapper(fIn, \"utf8\"):\n",
    "                    sent1, sent2, score = line.strip().split(\"\\t\")\n",
    "                    score = float(score)\n",
    "                    sts_data[filename][\"sentences1\"].append(sent1)\n",
    "                    sts_data[filename][\"sentences2\"].append(sent2)\n",
    "                    sts_data[filename][\"scores\"].append(score)\n",
    "\n",
    "for filename, data in sts_data.items():\n",
    "    test_evaluator = evaluation.EmbeddingSimilarityEvaluator(\n",
    "        data[\"sentences1\"],\n",
    "        data[\"sentences2\"],\n",
    "        data[\"scores\"],\n",
    "        batch_size=inference_batch_size,\n",
    "        name=filename,\n",
    "        show_progress_bar=False,\n",
    "    )\n",
    "    evaluators.append(test_evaluator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f04559b489d466b9d7bdf70f6a5ee73",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59165a4d35d84eeb8aa32523af34c4ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/11857 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "g:\\CYTech\\ING2\\ProjetLinkedin\\2024_02_25\\linkedin-work-experience-classification\\.venv\\lib\\site-packages\\sentence_transformers\\SentenceTransformer.py:775: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ..\\torch\\csrc\\utils\\tensor_new.cpp:278.)\n",
      "  labels = torch.tensor([example.label for example in batch])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-03-14 17:45:34 - MSE evaluation (lower = better) on talks-en-fr-dev.tsv.gz dataset in epoch 0 after 1000 steps:\n",
      "2024-03-14 17:45:34 - MSE (*100):\t0.899379\n",
      "2024-03-14 17:45:34 - Evaluating translation matching Accuracy on talks-en-fr-dev.tsv.gz dataset in epoch 0 after 1000 steps:\n",
      "2024-03-14 17:45:39 - Accuracy src2trg: 59.50\n",
      "2024-03-14 17:45:39 - Accuracy trg2src: 51.00\n",
      "2024-03-14 17:45:39 - Save model to output/make-multilingual-en-fr-2024-03-14_17-34-20\n",
      "2024-03-14 17:58:22 - MSE evaluation (lower = better) on talks-en-fr-dev.tsv.gz dataset in epoch 0 after 2000 steps:\n",
      "2024-03-14 17:58:22 - MSE (*100):\t0.828912\n",
      "2024-03-14 17:58:22 - Evaluating translation matching Accuracy on talks-en-fr-dev.tsv.gz dataset in epoch 0 after 2000 steps:\n",
      "2024-03-14 17:58:25 - Accuracy src2trg: 64.90\n",
      "2024-03-14 17:58:25 - Accuracy trg2src: 57.60\n",
      "2024-03-14 17:58:25 - Save model to output/make-multilingual-en-fr-2024-03-14_17-34-20\n",
      "2024-03-14 18:08:43 - MSE evaluation (lower = better) on talks-en-fr-dev.tsv.gz dataset in epoch 0 after 3000 steps:\n",
      "2024-03-14 18:08:43 - MSE (*100):\t0.784839\n",
      "2024-03-14 18:08:43 - Evaluating translation matching Accuracy on talks-en-fr-dev.tsv.gz dataset in epoch 0 after 3000 steps:\n",
      "2024-03-14 18:08:46 - Accuracy src2trg: 70.50\n",
      "2024-03-14 18:08:46 - Accuracy trg2src: 63.10\n",
      "2024-03-14 18:08:46 - Save model to output/make-multilingual-en-fr-2024-03-14_17-34-20\n",
      "2024-03-14 18:19:04 - MSE evaluation (lower = better) on talks-en-fr-dev.tsv.gz dataset in epoch 0 after 4000 steps:\n",
      "2024-03-14 18:19:04 - MSE (*100):\t0.756747\n",
      "2024-03-14 18:19:04 - Evaluating translation matching Accuracy on talks-en-fr-dev.tsv.gz dataset in epoch 0 after 4000 steps:\n",
      "2024-03-14 18:19:07 - Accuracy src2trg: 74.50\n",
      "2024-03-14 18:19:07 - Accuracy trg2src: 66.30\n",
      "2024-03-14 18:19:07 - Save model to output/make-multilingual-en-fr-2024-03-14_17-34-20\n",
      "2024-03-14 18:33:47 - MSE evaluation (lower = better) on talks-en-fr-dev.tsv.gz dataset in epoch 0 after 5000 steps:\n",
      "2024-03-14 18:33:47 - MSE (*100):\t0.717486\n",
      "2024-03-14 18:33:47 - Evaluating translation matching Accuracy on talks-en-fr-dev.tsv.gz dataset in epoch 0 after 5000 steps:\n",
      "2024-03-14 18:33:50 - Accuracy src2trg: 78.20\n",
      "2024-03-14 18:33:50 - Accuracy trg2src: 69.60\n",
      "2024-03-14 18:33:50 - Save model to output/make-multilingual-en-fr-2024-03-14_17-34-20\n",
      "2024-03-14 18:45:04 - MSE evaluation (lower = better) on talks-en-fr-dev.tsv.gz dataset in epoch 0 after 6000 steps:\n",
      "2024-03-14 18:45:04 - MSE (*100):\t0.697528\n",
      "2024-03-14 18:45:04 - Evaluating translation matching Accuracy on talks-en-fr-dev.tsv.gz dataset in epoch 0 after 6000 steps:\n",
      "2024-03-14 18:45:08 - Accuracy src2trg: 80.20\n",
      "2024-03-14 18:45:08 - Accuracy trg2src: 73.00\n",
      "2024-03-14 18:45:08 - Save model to output/make-multilingual-en-fr-2024-03-14_17-34-20\n",
      "2024-03-14 18:59:32 - MSE evaluation (lower = better) on talks-en-fr-dev.tsv.gz dataset in epoch 0 after 7000 steps:\n",
      "2024-03-14 18:59:32 - MSE (*100):\t0.678887\n",
      "2024-03-14 18:59:32 - Evaluating translation matching Accuracy on talks-en-fr-dev.tsv.gz dataset in epoch 0 after 7000 steps:\n",
      "2024-03-14 18:59:36 - Accuracy src2trg: 83.20\n",
      "2024-03-14 18:59:36 - Accuracy trg2src: 75.80\n",
      "2024-03-14 18:59:36 - Save model to output/make-multilingual-en-fr-2024-03-14_17-34-20\n",
      "2024-03-14 19:13:11 - MSE evaluation (lower = better) on talks-en-fr-dev.tsv.gz dataset in epoch 0 after 8000 steps:\n",
      "2024-03-14 19:13:11 - MSE (*100):\t0.659774\n",
      "2024-03-14 19:13:11 - Evaluating translation matching Accuracy on talks-en-fr-dev.tsv.gz dataset in epoch 0 after 8000 steps:\n",
      "2024-03-14 19:13:14 - Accuracy src2trg: 85.30\n",
      "2024-03-14 19:13:14 - Accuracy trg2src: 80.60\n",
      "2024-03-14 19:13:14 - Save model to output/make-multilingual-en-fr-2024-03-14_17-34-20\n",
      "2024-03-14 19:22:58 - MSE evaluation (lower = better) on talks-en-fr-dev.tsv.gz dataset in epoch 0 after 9000 steps:\n",
      "2024-03-14 19:22:58 - MSE (*100):\t0.630644\n",
      "2024-03-14 19:22:58 - Evaluating translation matching Accuracy on talks-en-fr-dev.tsv.gz dataset in epoch 0 after 9000 steps:\n",
      "2024-03-14 19:23:01 - Accuracy src2trg: 86.70\n",
      "2024-03-14 19:23:01 - Accuracy trg2src: 82.10\n",
      "2024-03-14 19:23:01 - Save model to output/make-multilingual-en-fr-2024-03-14_17-34-20\n",
      "2024-03-14 19:34:40 - MSE evaluation (lower = better) on talks-en-fr-dev.tsv.gz dataset in epoch 0 after 10000 steps:\n",
      "2024-03-14 19:34:40 - MSE (*100):\t0.602514\n",
      "2024-03-14 19:34:40 - Evaluating translation matching Accuracy on talks-en-fr-dev.tsv.gz dataset in epoch 0 after 10000 steps:\n",
      "2024-03-14 19:34:44 - Accuracy src2trg: 88.10\n",
      "2024-03-14 19:34:44 - Accuracy trg2src: 82.30\n",
      "2024-03-14 19:34:44 - Save model to output/make-multilingual-en-fr-2024-03-14_17-34-20\n",
      "2024-03-14 19:44:46 - MSE evaluation (lower = better) on talks-en-fr-dev.tsv.gz dataset in epoch 0 after 11000 steps:\n",
      "2024-03-14 19:44:46 - MSE (*100):\t0.589006\n",
      "2024-03-14 19:44:46 - Evaluating translation matching Accuracy on talks-en-fr-dev.tsv.gz dataset in epoch 0 after 11000 steps:\n",
      "2024-03-14 19:44:49 - Accuracy src2trg: 89.50\n",
      "2024-03-14 19:44:49 - Accuracy trg2src: 84.60\n",
      "2024-03-14 19:44:49 - Save model to output/make-multilingual-en-fr-2024-03-14_17-34-20\n",
      "2024-03-14 19:53:09 - MSE evaluation (lower = better) on talks-en-fr-dev.tsv.gz dataset after epoch 0:\n",
      "2024-03-14 19:53:09 - MSE (*100):\t0.574995\n",
      "2024-03-14 19:53:09 - Evaluating translation matching Accuracy on talks-en-fr-dev.tsv.gz dataset after epoch 0:\n",
      "2024-03-14 19:53:12 - Accuracy src2trg: 89.50\n",
      "2024-03-14 19:53:12 - Accuracy trg2src: 85.40\n",
      "2024-03-14 19:53:12 - Save model to output/make-multilingual-en-fr-2024-03-14_17-34-20\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "\n",
    "num_epochs = 1\n",
    "\n",
    "student_model.fit(\n",
    "    train_objectives=[(train_dataloader, train_loss)],\n",
    "    evaluator=evaluation.SequentialEvaluator(evaluators, main_score_function=lambda scores: np.mean(scores)),\n",
    "    epochs=num_epochs,\n",
    "    warmup_steps=num_warmup_steps,\n",
    "    evaluation_steps=num_evaluation_steps,\n",
    "    output_path=output_path,\n",
    "    save_best_model=True,\n",
    "    optimizer_params={\"lr\": 2e-5, \"eps\": 1e-6},\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc0c421a354f419f9fca7cd8f4491d8a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9fd59b8c2f3f48f09dba5efa387274e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/11857 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "g:\\CYTech\\ING2\\ProjetLinkedin\\2024_02_25\\linkedin-work-experience-classification\\.venv\\lib\\site-packages\\sentence_transformers\\SentenceTransformer.py:775: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ..\\torch\\csrc\\utils\\tensor_new.cpp:278.)\n",
      "  labels = torch.tensor([example.label for example in batch])\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "num_epochs = 1\n",
    "\n",
    "student_model.fit(\n",
    "    train_objectives=[(train_dataloader, train_loss)],\n",
    "    evaluator=evaluation.SequentialEvaluator(evaluators, main_score_function=lambda scores: np.mean(scores)),\n",
    "    epochs=num_epochs,\n",
    "    warmup_steps=num_warmup_steps,\n",
    "    evaluation_steps=num_evaluation_steps,\n",
    "    output_path=output_path,\n",
    "    save_best_model=True,\n",
    "    optimizer_params={\"lr\": 2e-5, \"eps\": 1e-6},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e00074fc9a246aea37aa5a8b4e43985",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-03-14 20:07:33 - Save model to C:\\Users\\robin\\AppData\\Local\\Temp\\tmpc4qovcem\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef4d77be4c2d4770b40beb8110556f30",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/438M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'https://huggingface.co/RobinMeneust/paraphrase-mpnet-base-v2-fr-en/commit/c1ad9933e0d4a46214dcb4acba15012271bd94ae'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from huggingface_hub import login\n",
    "\n",
    "login()\n",
    "# Push to the Hub\n",
    "student_model.save_to_hub(\"paraphrase-mpnet-base-v2-fr-en\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
