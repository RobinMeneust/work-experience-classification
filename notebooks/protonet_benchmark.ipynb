{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook to test ProtoNet performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup and data cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Change logs settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from datasets import disable_progress_bar\n",
    "disable_progress_bar() # Disable the \"Map\" progress bar during the tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and clean the dataset\n",
    "\n",
    "This dataset is not on the GitHub repository.\n",
    "It's composed of work experienced fetched from LinkedIn and labelled between 0 and 4 (0 if it's not related to AI and 4 if it is)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataFrame = pd.read_pickle(r'../data/7587_corrige.pkl')\n",
    "subset = dataFrame[['jobTitle', 'description', 'label']].copy()\n",
    "\n",
    "subset.reset_index(drop=True, inplace=True)\n",
    "subset.replace('', np.nan, inplace=True)\n",
    "subset.dropna(inplace=True)\n",
    "\n",
    "subset['text'] = subset['jobTitle'] + ' ' + subset['description']\n",
    "subset = subset[['text','label']]\n",
    "subset_label_transform = subset.copy()\n",
    "\n",
    "subset_label_transform['label'] = np.where((subset_label_transform[\"label\"] < 3) | (subset_label_transform[\"label\"].isna()), 0, 1)\n",
    "subset_label_transform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split the dataset in two subsets : the training and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from benchmark.utility import split_dataset\n",
    "train_set, test_set = split_dataset(subset_label_transform, 0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from benchmark.utility import save_to_json\n",
    "from benchmark.tests import n_shot_tests, input_length_tests, language_tests, model_tests, num_epochs_tests, frozen_ratio_tests\n",
    "from benchmark.train_eval_task import protonet_f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### N-shots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default SetFit uses the oversampling strategy and the Cosine Similarity loss. For instance if we have 8 positive and 8 negative examples then we have:\n",
    "\n",
    "|   | Y | Y | Y | Y | Y | Y | Y | Y | N | N | N | N | N | N | N | N |\n",
    "|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|\n",
    "| Y | + | + | + | + | + | + | + | + | - | - | - | - | - | - | - | - |\n",
    "| Y |   | + | + | + | + | + | + | + | - | - | - | - | - | - | - | - |\n",
    "| Y |   |   | + | + | + | + | + | + | - | - | - | - | - | - | - | - |\n",
    "| Y |   |   |   | + | + | + | + | + | - | - | - | - | - | - | - | - |\n",
    "| Y |   |   |   |   | + | + | + | + | - | - | - | - | - | - | - | - |\n",
    "| Y |   |   |   |   |   | + | + | + | - | - | - | - | - | - | - | - |\n",
    "| Y |   |   |   |   |   |   | + | + | - | - | - | - | - | - | - | - |\n",
    "| Y |   |   |   |   |   |   |   | + | - | - | - | - | - | - | - | - |\n",
    "| N |   |   |   |   |   |   |   |   | + | + | + | + | + | + | + | + |\n",
    "| N |   |   |   |   |   |   |   |   |   | + | + | + | + | + | + | + |\n",
    "| N |   |   |   |   |   |   |   |   |   |   | + | + | + | + | + | + |\n",
    "| N |   |   |   |   |   |   |   |   |   |   |   | + | + | + | + | + |\n",
    "| N |   |   |   |   |   |   |   |   |   |   |   |   | + | + | + | + |\n",
    "| N |   |   |   |   |   |   |   |   |   |   |   |   |   | + | + | + |\n",
    "| N |   |   |   |   |   |   |   |   |   |   |   |   |   |   | + | + |\n",
    "| N |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | + |\n",
    "\n",
    "- P = 2 * (8 + 7 + 6 + 5 + 4 + 3 + 2 + 1) \t= 72\n",
    "- N = 8 * 8 = 64 -> + 8 duplications \t\t= 72\n",
    "- Total = 72 + 72 = 144"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    \"n_shot\": [1, 2, 4, 6, 10],\n",
    "    \"n_iter\": 15,\n",
    "    \"n_max_iter_per_shot\": 10,\n",
    "    \"model\": \"sentence-transformers/paraphrase-mpnet-base-v2\",\n",
    "    \"loss\": \"Cosine\"\n",
    "}\n",
    "\n",
    "results, train_times, eval_times = n_shot_tests(params, train_set, test_set, few_shot_model_f1_function=protonet_f1_score)\n",
    "\n",
    "save_to_json(results, train_times, eval_times, params,  r'../results/protonet/n_shot')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    \"input_length_range\": [[0,5],[5,25],[25,50],[50,100],[100,200],[200,350]],\n",
    "\t\"n_shot\": 10,\n",
    "\t\"n_iter\": 100,\n",
    "\t\"model\": \"sentence-transformers/paraphrase-mpnet-base-v2\",\n",
    "\t\"loss\": \"Cosine\"\n",
    "}\n",
    "\n",
    "results, train_times, eval_times = input_length_tests(params, train_set, test_set, few_shot_model_f1_function=protonet_f1_score)\n",
    "\n",
    "save_to_json(results, train_times, eval_times, params,  r'../results/protonet/input_length')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    \"n_shot\": 10,\n",
    "\t\"lang\": ['fr','en'],\n",
    "\t\"n_iter\": 100,\n",
    "\t\"model\": \"sentence-transformers/paraphrase-mpnet-base-v2\",\n",
    "\t\"loss\": \"Cosine\"\n",
    "}\n",
    "\n",
    "results, train_times, eval_times = language_tests(params, train_set, test_set, few_shot_model_f1_function=protonet_f1_score)\n",
    "\n",
    "save_to_json(results, train_times, eval_times, params,  r'../results/protonet/language')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    \"n_shot\": 10,\n",
    "\t\"n_iter\": 100,\n",
    "\t\"loss\": \"Cosine\",\n",
    "    \"model\": {\n",
    "        # \"instructor-large\":\"hkunlp/instructor-large\",\n",
    "\t\t\"GIST-small-Embedding-v0\":\"avsolatorio/GIST-small-Embedding-v0\",\n",
    "\t\t\"gte-tiny\":\"TaylorAI/gte-tiny\",\n",
    "\t\t# \"all-mpnet-base-v2-table\":\"deepset/all-mpnet-base-v2-table\",\n",
    "  \t\t\"paraphrase-mpnet-base-v2\": \"sentence-transformers/paraphrase-mpnet-base-v2\",\n",
    "\t\t# \"all-mpnet-base-v2\":\"sentence-transformers/all-mpnet-base-v2\",\n",
    "    }\n",
    "}\n",
    "\n",
    "results, train_times, eval_times = model_tests(params, train_set, test_set, few_shot_model_f1_function=protonet_f1_score)\n",
    "\n",
    "save_to_json(results, train_times, eval_times, params,  r'../results/protonet/model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    \"n_shot\": 10,\n",
    "\t\"n_iter\": 100,\n",
    "\t\"model\": \"sentence-transformers/paraphrase-mpnet-base-v2\",\n",
    "\t\"loss\": \"Cosine\",\n",
    "    \"num_epochs\": [(1,0),(2,0),(4,0),(8,0),(16,0),(32,0)], # There is no classification head \n",
    "}\n",
    "\n",
    "results, train_times, eval_times = num_epochs_tests(params, train_set, test_set, few_shot_model_f1_function=protonet_f1_score)\n",
    "\n",
    "save_to_json(results, train_times, eval_times, params,  r'../results/protonet/num_epochs')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data sampling\n",
    "\n",
    "Run multiple tests with different training sets but the same parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# params = {\n",
    "# \t\"n_shot\": 10,\n",
    "# \t\"n_iter\": 100,\n",
    "# \t\"loss\": \"Cosine\",\n",
    "# \t\"model\": \"sentence-transformers/paraphrase-mpnet-base-v2\",\n",
    "# \t\"input_length_range\":[0,9],\n",
    "#     \"ratio_frozen_weights\": 0.5\n",
    "# }\n",
    "\n",
    "# results, train_times, eval_times = constant_params_tests(params, train_set, test_set, few_shot_model_f1_function=setfit_f1_score)\n",
    "\n",
    "# save_to_json(results, train_times, eval_times, params,  r'../results/protonet/data_sampling')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Frozen weights ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    \"n_shot\": 10,\n",
    "\t\"n_iter\": 100,\n",
    "\t\"model\": \"sentence-transformers/paraphrase-mpnet-base-v2\",\n",
    "\t\"loss\": \"Cosine\",\n",
    "    \"ratio_frozen_weights\": [0.1,0.3,0.5,0.7,0.9]\n",
    "}\n",
    "\n",
    "results, train_times, eval_times = frozen_ratio_tests(params, train_set, test_set, few_shot_model_f1_function=protonet_f1_score)\n",
    "\n",
    "save_to_json(results, train_times, eval_times, params,  r'../results/protonet/frozen_ratio')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
