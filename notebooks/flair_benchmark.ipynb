{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook to test Flair performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup and data cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Change logs settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from datasets import disable_progress_bar\n",
    "disable_progress_bar() # Disable the \"Map\" progress bar during the tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and clean the dataset\n",
    "\n",
    "This dataset is not on the GitHub repository.\n",
    "It's composed of work experienced fetched from LinkedIn and labelled between 0 and 4 (0 if it's not related to AI and 4 if it is)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataFrame = pd.read_pickle(r'../data/7587_corrige.pkl')\n",
    "subset = dataFrame[['jobTitle', 'description', 'label']].copy()\n",
    "\n",
    "subset.reset_index(drop=True, inplace=True)\n",
    "subset.replace('', np.nan, inplace=True)\n",
    "subset.dropna(inplace=True)\n",
    "\n",
    "subset['text'] = subset['jobTitle'] + ' ' + subset['description']\n",
    "subset = subset[['text','label']]\n",
    "subset_label_transform = subset.copy()\n",
    "\n",
    "subset_label_transform['label'] = np.where((subset_label_transform[\"label\"] < 3) | (subset_label_transform[\"label\"].isna()), 0, 1)\n",
    "subset_label_transform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split the dataset in two subsets : the training and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from benchmark.utility import split_dataset\n",
    "train_set, test_set = split_dataset(subset_label_transform, 0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "notebook_dir = os.path.dirname(os.path.abspath('__file__'))\n",
    "src_dir = os.path.abspath(os.path.join(notebook_dir, '..', 'src'))\n",
    "\n",
    "# Add 'src' directory to sys.path if it's not already there\n",
    "if src_dir not in sys.path:\n",
    "    sys.path.append(src_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from benchmark.utility import save_to_json\n",
    "from benchmark.tests import n_shot_tests\n",
    "from benchmark.train_eval_task import flair_f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### N-shots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default SetFit uses the oversampling strategy and the Cosine Similarity loss. For instance if we have 8 positive and 8 negative examples then we have:\n",
    "\n",
    "|   | Y | Y | Y | Y | Y | Y | Y | Y | N | N | N | N | N | N | N | N |\n",
    "|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|\n",
    "| Y | + | + | + | + | + | + | + | + | - | - | - | - | - | - | - | - |\n",
    "| Y |   | + | + | + | + | + | + | + | - | - | - | - | - | - | - | - |\n",
    "| Y |   |   | + | + | + | + | + | + | - | - | - | - | - | - | - | - |\n",
    "| Y |   |   |   | + | + | + | + | + | - | - | - | - | - | - | - | - |\n",
    "| Y |   |   |   |   | + | + | + | + | - | - | - | - | - | - | - | - |\n",
    "| Y |   |   |   |   |   | + | + | + | - | - | - | - | - | - | - | - |\n",
    "| Y |   |   |   |   |   |   | + | + | - | - | - | - | - | - | - | - |\n",
    "| Y |   |   |   |   |   |   |   | + | - | - | - | - | - | - | - | - |\n",
    "| N |   |   |   |   |   |   |   |   | + | + | + | + | + | + | + | + |\n",
    "| N |   |   |   |   |   |   |   |   |   | + | + | + | + | + | + | + |\n",
    "| N |   |   |   |   |   |   |   |   |   |   | + | + | + | + | + | + |\n",
    "| N |   |   |   |   |   |   |   |   |   |   |   | + | + | + | + | + |\n",
    "| N |   |   |   |   |   |   |   |   |   |   |   |   | + | + | + | + |\n",
    "| N |   |   |   |   |   |   |   |   |   |   |   |   |   | + | + | + |\n",
    "| N |   |   |   |   |   |   |   |   |   |   |   |   |   |   | + | + |\n",
    "| N |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | + |\n",
    "\n",
    "- P = 2 * (8 + 7 + 6 + 5 + 4 + 3 + 2 + 1) \t= 72\n",
    "- N = 8 * 8 = 64 -> + 8 duplications \t\t= 72\n",
    "- Total = 72 + 72 = 144"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    \"n_shot\": [1, 2, 4, 6, 10],\n",
    "    \"n_iter\": 5,\n",
    "    \"n_max_iter_per_shot\": 5,\n",
    "    \"model\": \"flair-base\",\n",
    "    \"loss\": \"CrossEntropyLoss\"\n",
    "}\n",
    "\n",
    "results, train_times, eval_times = n_shot_tests(params, train_set, test_set, few_shot_model_f1_function=flair_f1_score)\n",
    "\n",
    "save_to_json(results, train_times, eval_times, params,  r'../results/flair/n_shot')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
