{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook to test how to use BERT and PyTorch with CUDA for FSL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load dataset and transform data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Stagiaire ingénieur en intelligence artificiel...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Stagiaire en développement logiciel Développem...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Stagiaire en développement Web Création et évo...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Stagiaire en développement Web Portage d’une a...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Développeur Data / IA Développement d'applicat...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11281</th>\n",
       "      <td>Opérateur production Montage de transmission a...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11282</th>\n",
       "      <td>Opérateur production Montage de transmission a...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11283</th>\n",
       "      <td>Technicien réparation informatique Reparation ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11284</th>\n",
       "      <td>Technicien réparation Reparation &amp; maintenance...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11286</th>\n",
       "      <td>Développeur web freelance Webdesign &amp; Infographie</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5167 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text  label\n",
       "2      Stagiaire ingénieur en intelligence artificiel...      1\n",
       "3      Stagiaire en développement logiciel Développem...      0\n",
       "4      Stagiaire en développement Web Création et évo...      0\n",
       "5      Stagiaire en développement Web Portage d’une a...      0\n",
       "6      Développeur Data / IA Développement d'applicat...      1\n",
       "...                                                  ...    ...\n",
       "11281  Opérateur production Montage de transmission a...      0\n",
       "11282  Opérateur production Montage de transmission a...      0\n",
       "11283  Technicien réparation informatique Reparation ...      0\n",
       "11284  Technicien réparation Reparation & maintenance...      0\n",
       "11286  Développeur web freelance Webdesign & Infographie      0\n",
       "\n",
       "[5167 rows x 2 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "dataFrame = pd.read_pickle(r'../data/7587_corrige.pkl')\n",
    "subset = dataFrame[['jobTitle', 'description', 'label']].copy()\n",
    "\n",
    "subset.reset_index(drop=True, inplace=True)\n",
    "subset.replace('', np.nan, inplace=True)\n",
    "subset.dropna(inplace=True)\n",
    "\n",
    "subset['text'] = subset['jobTitle'] + ' ' + subset['description']\n",
    "subset = subset[['text','label']]\n",
    "subset_label_transform = subset.copy()\n",
    "\n",
    "subset_label_transform['label'] = np.where((subset_label_transform[\"label\"] < 3) | (subset_label_transform[\"label\"].isna()), 0, 1)\n",
    "subset_label_transform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split between training and test set and create support set and load BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce03ce6a493e46c0a6f36cd3c50ac759",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/16 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d47fdcce8454638bb5e17997150eb02",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/16 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from utility import split_dataset\n",
    "from protoNet import gen_support_set, get_tokenizer_and_model\n",
    "from utility import get_n_shot_dataset\n",
    "from datasets import Dataset\n",
    "\n",
    "n_shots = 8\n",
    "\n",
    "train_set, test_set = split_dataset(subset_label_transform, 0.2)\n",
    "train_set = get_n_shot_dataset(train_set, n_shots)\n",
    "train_set = Dataset.from_pandas(train_set)\n",
    "test_set = Dataset.from_pandas(test_set)\n",
    "\n",
    "tokenizer, model = get_tokenizer_and_model()\n",
    "\n",
    "support_set = gen_support_set(n_shots, tokenizer, train_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training using prototypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  1 / 20 ... Training loss: 0.50\n",
      "Epoch:  2 / 20 ... Training loss: 0.49\n",
      "Epoch:  3 / 20 ... Training loss: 0.50\n",
      "Epoch:  4 / 20 ... Training loss: 0.48\n",
      "Epoch:  5 / 20 ... Training loss: 0.49\n",
      "Epoch:  6 / 20 ... Training loss: 0.48\n",
      "Epoch:  7 / 20 ... Training loss: 0.47\n",
      "Epoch:  8 / 20 ... Training loss: 0.45\n",
      "Epoch:  9 / 20 ... Training loss: 0.44\n",
      "Epoch:  10 / 20 ... Training loss: 0.45\n",
      "Epoch:  11 / 20 ... Training loss: 0.40\n",
      "Epoch:  12 / 20 ... Training loss: 0.37\n",
      "Epoch:  13 / 20 ... Training loss: 0.32\n",
      "Epoch:  14 / 20 ... Training loss: 0.30\n",
      "Epoch:  15 / 20 ... Training loss: 0.24\n",
      "Epoch:  16 / 20 ... Training loss: 0.25\n",
      "Epoch:  17 / 20 ... Training loss: 0.23\n",
      "Epoch:  18 / 20 ... Training loss: 0.27\n",
      "Epoch:  19 / 20 ... Training loss: 0.20\n",
      "Epoch:  20 / 20 ... Training loss: 0.21\n"
     ]
    }
   ],
   "source": [
    "from protoNet import protonet_train, eval\n",
    "\n",
    "model = protonet_train(support_set, train_set, tokenizer, model, num_epochs=20, batch_size=16, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval: 1 / 10\n",
      "Eval: 2 / 10\n",
      "Eval: 3 / 10\n",
      "Eval: 4 / 10\n",
      "Eval: 5 / 10\n",
      "Eval: 6 / 10\n",
      "Eval: 7 / 10\n",
      "Eval: 8 / 10\n",
      "Eval: 9 / 10\n",
      "Eval: 10 / 10\n",
      "F1 score: 0.31\n"
     ]
    }
   ],
   "source": [
    "f1_score = eval(test_set, tokenizer, model, support_set, verbose=True)\n",
    "print(f\"F1 score: {f1_score:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "expected:  [1, 0] predictions:  [1, 0]\n"
     ]
    }
   ],
   "source": [
    "from protoNet import predict\n",
    "\n",
    "pred1 = predict(tokenizer, model, \"Mon rôle chez DreamQuark, est de résoudre les problématiques des différents acteurs autour de la\\nbanque et assurance (Churn, upsale, cross-sale etc.) à travers des techniques de Machine\\nLearning/Deep learning et analyse statistique.\\n\\n● Contribution à l'amélioration de Brain, la plateforme d'Auto-ML de Dreamquark, en développant de nouvelles features à l'aide du framework Pytorch, Scikit-learn, Numpy, Pandas, FastApi, Docker, Kubernetes et CircleCi\\n\\n● Développement d'un package Time Series avec l'intégration de module automatique de preprocessing et module de training avec des réseaux de neurone TCN (Temporal Convolutional Network)\\n\\n● Développement d'un moteur de data-preparation scalable à l'horizontal compatible Pandas et Dask, s'inspirant de la philosophe Pandas et scikit-learn pipeline permettant de rendre reproductible les codes jupyter en production.\\n\\nStack Technique :\\n\\nPython, Pytorch, Scikit-learn, Numpy, Docker, Kubernetes, Circleci, Dask, FastApi, Dask, Azure, Circle\\nCi, Prefect, Alembic, SqlAlchemy, Postgresql'\", support_set)\n",
    "pred2 = predict(tokenizer, model, \"• Utilisation de Flask et d’Elasticsearch afin de créer une API\\nREST pour faire des recherches sur des régions de\\nplanètes.\\n\\n• Conception d'une application web avec Vue.js et Quasar\\nutilisant cette API, avec visualisation 3D des données.\", support_set)\n",
    "\n",
    "print(\"expected: \", [1,0], \"predictions: \", [pred1,pred2])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
