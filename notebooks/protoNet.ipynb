{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook to test how to use BERT and PyTorch with CUDA for FSL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check CUDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "if torch.cuda.is_available():    \n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 768])\n"
     ]
    }
   ],
   "source": [
    "def get_tokenizer_and_model(source=\"bert-base-multilingual-cased\"):\n",
    "    tokenizer = BertTokenizer.from_pretrained(source)\n",
    "    embedding_model = BertModel.from_pretrained(source)\n",
    "    embedding_model.to(device)\n",
    "    embedding_model.cuda()\n",
    "    return tokenizer, embedding_model\n",
    "\n",
    "tokenizer, bert = get_tokenizer_and_model()\n",
    "\n",
    "encoded_input = tokenizer([\"test 1\",\"test 2\"], return_tensors='pt')\n",
    "encoded_input.to(device)\n",
    "output = bert(**encoded_input)\n",
    "print(output[\"pooler_output\"].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load dataset and transform data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Stagiaire ingénieur en intelligence artificiel...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Stagiaire en développement logiciel Développem...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Stagiaire en développement Web Création et évo...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Stagiaire en développement Web Portage d’une a...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Développeur Data / IA Développement d'applicat...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11281</th>\n",
       "      <td>Opérateur production Montage de transmission a...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11282</th>\n",
       "      <td>Opérateur production Montage de transmission a...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11283</th>\n",
       "      <td>Technicien réparation informatique Reparation ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11284</th>\n",
       "      <td>Technicien réparation Reparation &amp; maintenance...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11286</th>\n",
       "      <td>Développeur web freelance Webdesign &amp; Infographie</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5167 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text  label\n",
       "2      Stagiaire ingénieur en intelligence artificiel...      1\n",
       "3      Stagiaire en développement logiciel Développem...      0\n",
       "4      Stagiaire en développement Web Création et évo...      0\n",
       "5      Stagiaire en développement Web Portage d’une a...      0\n",
       "6      Développeur Data / IA Développement d'applicat...      1\n",
       "...                                                  ...    ...\n",
       "11281  Opérateur production Montage de transmission a...      0\n",
       "11282  Opérateur production Montage de transmission a...      0\n",
       "11283  Technicien réparation informatique Reparation ...      0\n",
       "11284  Technicien réparation Reparation & maintenance...      0\n",
       "11286  Développeur web freelance Webdesign & Infographie      0\n",
       "\n",
       "[5167 rows x 2 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataFrame = pd.read_pickle(r'../data/7587_corrige.pkl')\n",
    "subset = dataFrame[['jobTitle', 'description', 'label']].copy()\n",
    "\n",
    "subset.reset_index(drop=True, inplace=True)\n",
    "subset.replace('', np.nan, inplace=True)\n",
    "subset.dropna(inplace=True)\n",
    "\n",
    "subset['text'] = subset['jobTitle'] + ' ' + subset['description']\n",
    "subset = subset[['text','label']]\n",
    "subset_label_transform = subset.copy()\n",
    "\n",
    "subset_label_transform['label'] = np.where((subset_label_transform[\"label\"] < 3) | (subset_label_transform[\"label\"].isna()), 0, 1)\n",
    "subset_label_transform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split between training and test set and truncate the dataset to simulate few-shot context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_train_test_n_shot(dataset, n_samples_per_class):\n",
    "    train_set = dataset.groupby('label').head(n_samples_per_class)\n",
    "    test_set = dataset.drop(train_set.index)\n",
    "    return train_set, test_set\n",
    "\n",
    "train_set, test_set = split_train_test_n_shot(subset, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create support set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_shots = min(10,len(train_set)//2) # Number of samples per class in the support set\n",
    "\n",
    "def gen_support_set(n_shots, tokenizer, dataset):   \n",
    "    shuffled_dataset = dataset.sample(frac = 1)\n",
    "    support_set = {}\n",
    "    for t in [0,1]: # class 0 and class 1 (not related to AI and related to AI)\n",
    "        current_target_dataset = shuffled_dataset[shuffled_dataset[\"label\"] == t]\n",
    "        support_set[t] = []\n",
    "        for i in range(n_shots):\n",
    "            encoded_input = tokenizer(current_target_dataset.iloc[i][\"text\"], return_tensors='pt', truncation=True)\n",
    "            encoded_input.to(device)\n",
    "            support_set[t].append(encoded_input)\n",
    "    return support_set\n",
    "    \n",
    "support_set = gen_support_set(n_shots, tokenizer, train_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get prototypes from the support set (one prototype per class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prototypes_support_set(support_set, bert):\n",
    "    prototypes_support_set = {}\n",
    "    for t in support_set.keys():\n",
    "        embeddings_support_set = []\n",
    "        for i in range(len(support_set[t])):\n",
    "            output = bert(**(support_set[t][i]))[\"pooler_output\"]\n",
    "            embeddings_support_set.append(output)\n",
    "        prototypes_support_set[t] = torch.mean(torch.stack(embeddings_support_set), axis=0)\n",
    "    return prototypes_support_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(tokenizer, bert, instance, support_set):\n",
    "    bert.eval()\n",
    "    encoded_input = tokenizer(instance, return_tensors='pt', truncation=True)\n",
    "    encoded_input.to(device)\n",
    "    embedding = bert(**encoded_input)[\"pooler_output\"]\n",
    "    similarities = []\n",
    "    \n",
    "    prototypes_support_set = get_prototypes_support_set(support_set, bert)\n",
    "    \n",
    "    for key in prototypes_support_set.keys():\n",
    "        similarity_current_key = torch.nn.functional.cosine_similarity(embedding, prototypes_support_set[key])\n",
    "        similarities.append(similarity_current_key)\n",
    "    return list(prototypes_support_set.keys())[torch.argmax(torch.stack(similarities))] # Take the closest element of all classes and return its class label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_batches(training_set, tokenizer, batch_size):\n",
    "    batches = []\n",
    "    shuffled_set = training_set.sample(frac=1)\n",
    "\n",
    "    nb_batches = len(shuffled_set) // batch_size\n",
    "    \n",
    "    k = 0\n",
    "    len_shuffled_set = len(shuffled_set)\n",
    "    unprocessed_data = shuffled_set[\"text\"].tolist()\n",
    "    \n",
    "    for i in range(nb_batches):\n",
    "        j = 0\n",
    "        labels = []\n",
    "        start = i * batch_size\n",
    "        end = start + batch_size\n",
    "        unprocessed_batch = unprocessed_data[start:end]\n",
    "        inputs = tokenizer(unprocessed_batch, return_tensors='pt', padding=True, truncation=True)\n",
    "\n",
    "        while(j<batch_size and k<len_shuffled_set):\n",
    "            labels.append(shuffled_set.iloc[k][\"label\"])\n",
    "            k += 1\n",
    "            j += 1\n",
    "        batches.append((inputs, labels))\n",
    "            \n",
    "    return batches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Freeze some weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "freeze_first_params_ratio = 0.7\n",
    "nb_frozen_params = int(freeze_first_params_ratio * len(list(bert.named_parameters())))\n",
    "\n",
    "for name, param in list(bert.named_parameters())[0:nb_frozen_params+1]: \n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torcheval.metrics.functional import binary_f1_score\n",
    "\n",
    "def eval(test_set, tokenizer, bert, support_set):   \n",
    "    bert.eval()\n",
    "\n",
    "    predictions = []\n",
    "    expected = []\n",
    "\n",
    "    batches = gen_batches(test_set, tokenizer, 16)\n",
    "    # b = 0\n",
    "\n",
    "    prototypes_support_set = get_prototypes_support_set(support_set, bert)\n",
    "\n",
    "    for batch in batches:\n",
    "        inputs, labels = batch\n",
    "        # print(\"Batch: \", b, \"/\",len(batches))\n",
    "        # b += 1\n",
    "        inputs.to(device)\n",
    "        bert_output = bert(**inputs)[\"pooler_output\"]\n",
    "            \n",
    "        for i in range(len(bert_output)):\n",
    "            embedding = torch.unsqueeze(bert_output[i],0)\n",
    "            similarities = []\n",
    "            for key in prototypes_support_set.keys():\n",
    "                similarity_current_key = torch.nn.functional.cosine_similarity(embedding, prototypes_support_set[key])\n",
    "                similarities.append(similarity_current_key)\n",
    "            predictions.append(torch.tensor(list(prototypes_support_set.keys())[torch.argmax(torch.stack(similarities))])) # Take the closest element of all classes and return its class label\n",
    "            expected.append(torch.tensor(labels[i]))\n",
    "\n",
    "    predictions = torch.stack(predictions)\n",
    "    expected = torch.stack(expected)\n",
    "\n",
    "    return binary_f1_score(predictions, expected).item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fine-tune BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0 / 20\n",
      "training loss: 0.71\n",
      "Epoch:  1 / 20\n",
      "training loss: 0.72\n",
      "Epoch:  2 / 20\n",
      "training loss: 0.62\n",
      "Epoch:  3 / 20\n",
      "training loss: 0.61\n",
      "Epoch:  4 / 20\n",
      "training loss: 0.57\n",
      "Epoch:  5 / 20\n",
      "training loss: 0.49\n",
      "Epoch:  6 / 20\n",
      "training loss: 0.43\n",
      "Epoch:  7 / 20\n",
      "training loss: 0.41\n",
      "Epoch:  8 / 20\n",
      "training loss: 0.32\n",
      "Epoch:  9 / 20\n",
      "training loss: 0.33\n",
      "Epoch:  10 / 20\n",
      "training loss: 0.33\n",
      "Epoch:  11 / 20\n",
      "training loss: 0.36\n",
      "Epoch:  12 / 20\n",
      "training loss: 0.34\n",
      "Epoch:  13 / 20\n",
      "training loss: 0.33\n",
      "Epoch:  14 / 20\n",
      "training loss: 0.31\n",
      "Epoch:  15 / 20\n",
      "training loss: 0.33\n",
      "Epoch:  16 / 20\n",
      "training loss: 0.26\n",
      "Epoch:  17 / 20\n",
      "training loss: 0.29\n",
      "Epoch:  18 / 20\n",
      "training loss: 0.26\n",
      "Epoch:  19 / 20\n",
      "training loss: 0.24\n"
     ]
    }
   ],
   "source": [
    "def protonet_train(support_set, train_set, tokenizer, embedding_model, n_epochs=20, optimizer = torch.optim.AdamW(bert.parameters(), lr=1e-5)):\n",
    "    torch.cuda.empty_cache()\n",
    "    embedding_model.zero_grad()\n",
    "\n",
    "    try:\n",
    "        embedding_model.train()\n",
    "        for epoch in range(n_epochs):\n",
    "            batches = gen_batches(train_set, tokenizer, 16)\n",
    "            print(\"Epoch: \", epoch, \"/\",n_epochs)\n",
    "            epoch_mean_loss = 0\n",
    "            for batch in batches:\n",
    "                optimizer.zero_grad()\n",
    "                inputs, labels = batch\n",
    "                inputs.to(device)\n",
    "                embedding_model_output = embedding_model(**inputs)[\"pooler_output\"]\n",
    "                losses = []           \n",
    "                \n",
    "                embeddings_support_set = get_prototypes_support_set(support_set, embedding_model)\n",
    "            \n",
    "                for i in range(len(embedding_model_output)):\n",
    "                    input2 = torch.unsqueeze(embedding_model_output[i],0)\n",
    "                    input2.to(device)\n",
    "                    for j in embeddings_support_set.keys():\n",
    "                        current_class_support_data = embeddings_support_set[j]\n",
    "                        target = torch.tensor([1.0]) if j == labels[i] else torch.tensor([-1.0])\n",
    "                        target = target.to(device)\n",
    "                        losses.append(torch.nn.functional.cosine_embedding_loss(current_class_support_data, input2, target))\n",
    "                loss = torch.mean(torch.stack(losses))\n",
    "                epoch_mean_loss += loss.item()\n",
    "                            \n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "            \n",
    "            epoch_mean_loss /= len(batches)\n",
    "            print(f\"training loss: {epoch_mean_loss:.2f}\")\n",
    "    finally:\n",
    "        torch.cuda.empty_cache()\n",
    "    return embedding_model\n",
    "\n",
    "bert = protonet_train(support_set, train_set, tokenizer, bert)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "expected:  [1, 0] predictions:  [1, 1]\n"
     ]
    }
   ],
   "source": [
    "pred1 = predict(tokenizer, bert,\"Mon rôle chez DreamQuark, est de résoudre les problématiques des différents acteurs autour de la\\nbanque et assurance (Churn, upsale, cross-sale etc.) à travers des techniques de Machine\\nLearning/Deep learning et analyse statistique.\\n\\n● Contribution à l'amélioration de Brain, la plateforme d'Auto-ML de Dreamquark, en développant de nouvelles features à l'aide du framework Pytorch, Scikit-learn, Numpy, Pandas, FastApi, Docker, Kubernetes et CircleCi\\n\\n● Développement d'un package Time Series avec l'intégration de module automatique de preprocessing et module de training avec des réseaux de neurone TCN (Temporal Convolutional Network)\\n\\n● Développement d'un moteur de data-preparation scalable à l'horizontal compatible Pandas et Dask, s'inspirant de la philosophe Pandas et scikit-learn pipeline permettant de rendre reproductible les codes jupyter en production.\\n\\nStack Technique :\\n\\nPython, Pytorch, Scikit-learn, Numpy, Docker, Kubernetes, Circleci, Dask, FastApi, Dask, Azure, Circle\\nCi, Prefect, Alembic, SqlAlchemy, Postgresql'\",support_set)\n",
    "pred2 = predict(tokenizer, bert,\"• Utilisation de Flask et d’Elasticsearch afin de créer une API\\nREST pour faire des recherches sur des régions de\\nplanètes.\\n\\n• Conception d'une application web avec Vue.js et Quasar\\nutilisant cette API, avec visualisation 3D des données.\",support_set)\n",
    "\n",
    "print(\"expected: \", [1,0], \"predictions: \", [pred1,pred2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score: 0.86\n"
     ]
    }
   ],
   "source": [
    "print(f\"F1 score: {eval(test_set.head(1000), tokenizer, bert, support_set):.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
